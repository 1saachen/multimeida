import os
import torch
import torch.nn as nn
import torch.nn.functional as F
import pandas as pd
import numpy as np
from torch_geometric.data import Data, Batch
from torch_geometric.nn import GraphConv, global_mean_pool
import transformers
from transformers import AutoTokenizer, AutoModel, AutoConfig
import spacy
import nltk
import re
from textstat.textstat import textstat
from collections import Counter
import hashlib
import json
from sklearn.preprocessing import StandardScaler, MinMaxScaler
import pickle

# ---------------------------
# ---- ä»é…ç½®æ–‡ä»¶è¯»å–é…ç½® ----
# ---------------------------
def load_config(config_path="model_config.json"):
    """ä»é…ç½®æ–‡ä»¶åŠ è½½æ‰€æœ‰é…ç½®"""
    with open(config_path, 'r') as f:
        config = json.load(f)
    
    # è®¾ç½®å…¨å±€å˜é‡
    global MODEL_NAME, MAX_LENGTH, GRAPH_IN_DIM, GRAPH_HIDDEN, GRAPH_OUT
    global NUM_FEATURES, PROMPT_RELEVANCE_WEIGHT, DROPOUT, FREEZE_BERT_LAYERS
    global MIN_SCORE, MAX_SCORE
    
    MODEL_NAME = config.get("MODEL_NAME", "roberta-base")
    MAX_LENGTH = config.get("MAX_LENGTH", 256)
    GRAPH_IN_DIM = config.get("GRAPH_IN_DIM", 2)
    GRAPH_HIDDEN = config.get("GRAPH_HIDDEN", 64)
    GRAPH_OUT = config.get("GRAPH_OUT", 256)
    NUM_FEATURES = config.get("NUM_FEATURES", 20)
    PROMPT_RELEVANCE_WEIGHT = config.get("PROMPT_RELEVANCE_WEIGHT", 0.15)
    DROPOUT = config.get("DROPOUT", 0.3)
    FREEZE_BERT_LAYERS = config.get("FREEZE_BERT_LAYERS", 6)
    MIN_SCORE = config.get("min_score", 1.0)
    MAX_SCORE = config.get("max_score", 5.0)
    
    return config

# åŠ è½½é…ç½®
config = load_config()

print("ğŸ”§ é…ç½®ä¿¡æ¯:")
print(f"  - æ¨¡å‹: {MODEL_NAME}")
print(f"  - åˆ†æ•°èŒƒå›´: {MIN_SCORE}-{MAX_SCORE}")
print(f"  - å›¾ç¼–ç å™¨è¾“å‡º: {GRAPH_OUT}")
print(f"  - ç‰¹å¾æ•°é‡: {NUM_FEATURES}")
print(f"  - Promptç›¸å…³åº¦æƒé‡: {PROMPT_RELEVANCE_WEIGHT}")
print(f"  - Dropout: {DROPOUT}")

# ---------------------------
# ---- ä¸è®­ç»ƒä»£ç å®Œå…¨ä¸€è‡´çš„ç‰¹å¾æå–å™¨ ----
# ---------------------------
class FeatureExtractor:
    def __init__(self):
        self.dale_chall_common_words = self._load_dale_chall_words()
        self.NUM_FEATURES = NUM_FEATURES
        self.feature_names = [
            'char_count', 'word_count', 'comma_count', 'unique_words',
            'proper_nouns', 'determiners', 'nouns', 'adverbs', 'adjectives', 'prepositions',
            'gunning_fog', 'smog_index', 'rix_index', 'dale_chall', 'word_types',
            'sentence_count', 'long_words', 'complex_words', 'uncommon_words', 'difficult_words'
        ]
    
    def _load_dale_chall_words(self):
        """åŠ è½½å¸¸ç”¨è¯è¡¨ - ä¸è®­ç»ƒä»£ç å®Œå…¨ä¸€è‡´"""
        dale_chall_url = "https://gist.githubusercontent.com/Abhishek-P/e00edcc6f508640fe24f263f5836a7dc/raw/166225e09fb8b554deff37ec344ad5ca40dab2fb/dale-chall-3000-words.txt"
        try:
            import requests
            response = requests.get(dale_chall_url, timeout=10)
            if response.status_code == 200:
                return set(response.text.splitlines())
        except:
            pass
        return set()
    
    def safe_word_tokenize(self, text):
        """å®‰å…¨çš„åˆ†è¯å‡½æ•° - ä¸è®­ç»ƒä»£ç å®Œå…¨ä¸€è‡´"""
        try:
            return nltk.word_tokenize(str(text))
        except:
            try:
                return str(text).split()
            except:
                return []
    
    def extract_features(self, text):
        """æå–20ç»´è¯­è¨€å­¦ç‰¹å¾ - ä¸è®­ç»ƒä»£ç å®Œå…¨ä¸€è‡´"""
        if pd.isna(text) or text == "" or text is None:
            return np.zeros(self.NUM_FEATURES, dtype=float), {}
        
        try:
            text_str = str(text)
            tokens = self.safe_word_tokenize(text_str)
            words = [t for t in tokens if re.match(r"\w+", t)]
            n_chars = len(text_str)
            n_words = len(words)
            n_sent = max(1, text_str.count('.') + text_str.count('!') + text_str.count('?'))
            uniq = len(set(w.lower() for w in words))
            
            # è¯æ€§æ ‡æ³¨
            try:
                pos = nltk.pos_tag(words)
                pos_counts = Counter(tag for _, tag in pos)
            except:
                pos_counts = Counter()

            # åŸºç¡€ç‰¹å¾
            ch = n_chars
            w = n_words
            co = text_str.count(',')
            uw = uniq

            # è¯æ€§ç‰¹å¾
            nnp = pos_counts.get('NNP', 0)
            dt = pos_counts.get('DT', 0)
            nn = pos_counts.get('NN', 0)
            rb = pos_counts.get('RB', 0)
            jj = pos_counts.get('JJ', 0)
            inn = pos_counts.get('IN', 0)

            # å¯è¯»æ€§ç‰¹å¾
            try:
                fog = textstat.gunning_fog(text_str) if n_words > 0 else 0
            except:
                fog = 0
            try:
                smog = textstat.smog_index(text_str) if n_words > 0 else 0
            except:
                smog = 0
            try:
                rix = textstat.rix(text_str) if n_words > 0 else 0
            except:
                rix = 0
            try:
                dc = textstat.dale_chall_readability_score(text_str) if n_words > 0 else 0
            except:
                dc = 0
                
            wt = len(set(words))
            s = n_sent
            lw = sum(1 for w in words if len(w) > 6)
            
            try:
                cw = sum(1 for w in words if textstat.syllable_count(w) > 2)
            except:
                cw = 0
                
            nbw = sum(1 for w in words if w.lower() not in self.dale_chall_common_words)
            
            try:
                dw = sum(1 for w in words if len(textstat.difficult_words_list([w])) > 0)
            except:
                dw = 0

            feats = [ch, w, co, uw, nnp, dt, nn, rb, jj, inn, fog, smog, rix, dc, wt, s, lw, cw, nbw, dw]
            
            # åˆ›å»ºç‰¹å¾è¯¦ç»†å­—å…¸
            feature_details = dict(zip(self.feature_names, feats))
            
            return np.array(feats, dtype=float), feature_details
            
        except Exception as e:
            print(f"Error computing features: {e}")
            return np.zeros(self.NUM_FEATURES, dtype=float), {name: 0 for name in self.feature_names}

# ---------------------------
# ---- ä¸è®­ç»ƒä»£ç å®Œå…¨ä¸€è‡´çš„å›¾æ„å»ºå‡½æ•° ----
# ---------------------------
def build_graph_from_text(text: str, nlp, max_nodes: int = 256) -> Data:
    """æ„å»ºä¾èµ–å›¾ - ä¸è®­ç»ƒä»£ç å®Œå…¨ä¸€è‡´"""
    doc = nlp(text)
    nodes = list(doc)[:max_nodes]
    if len(nodes) == 0:
        x = torch.zeros((1, GRAPH_IN_DIM), dtype=torch.float32)
        edge_index = torch.tensor([[0],[0]], dtype=torch.long)
        return Data(x=x, edge_index=edge_index)

    edges = []
    for token in nodes:
        head_idx = token.head.i
        if token.i != head_idx and head_idx < max_nodes:
            edges.append((token.i, head_idx))
    if len(edges) == 0:
        edges = [(0, 0)]

    pos_ids = [int(tok.pos) for tok in nodes]
    is_root = [1 if tok.dep_ == "ROOT" else 0 for tok in nodes]
    x = torch.tensor(list(zip(pos_ids, is_root)), dtype=torch.float32)
    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()
    
    # è®¡ç®—å›¾ç»“æ„æŒ‡æ ‡
    graph_metrics = {
        'num_nodes': len(nodes),
        'num_edges': len(edges),
        'avg_dependency_distance': np.mean([abs(edge[0] - edge[1]) for edge in edges]) if edges else 0,
        'root_nodes': sum(is_root),
        'syntactic_complexity': len(edges) / max(len(nodes), 1)
    }
    
    return Data(x=x, edge_index=edge_index), graph_metrics

# ---------------------------
# ---- ä¸è®­ç»ƒä»£ç å®Œå…¨ä¸€è‡´çš„æ¨¡å‹ç»„ä»¶ ----
# ---------------------------
class FixedPromptRelevanceModule(nn.Module):
    """ä¿®å¤çš„Promptç›¸å…³åº¦è¯„ä¼°æ¨¡å— - ä¸è®­ç»ƒä»£ç å®Œå…¨ä¸€è‡´"""
    def __init__(self, hidden_dim=128, dropout=0.1):
        super().__init__()
        self.hidden_dim = hidden_dim
        
        # ä½¿ç”¨æ›´ç®€å•çš„æ¶æ„ï¼Œé¿å…é¥±å’Œ
        self.essay_pool = nn.AdaptiveAvgPool1d(1)
        self.prompt_pool = nn.AdaptiveAvgPool1d(1)
        
        # ç›¸ä¼¼åº¦è®¡ç®— - ä½¿ç”¨æ›´å°çš„ç½‘ç»œ
        self.similarity_net = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim // 4),
            nn.Tanh(),  # ä½¿ç”¨Tanhé¿å…é¥±å’Œ
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 4, hidden_dim // 8),
            nn.Tanh(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 8, 1),
            nn.Sigmoid()
        )
        
        # æ›´ä¿å®ˆçš„åˆå§‹åŒ–
        self._init_weights()
        
    def _init_weights(self):
        for module in self.modules():
            if isinstance(module, nn.Linear):
                # ä½¿ç”¨æ›´å°çš„åˆå§‹åŒ–æƒé‡
                nn.init.normal_(module.weight, mean=0.0, std=0.02)
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0.0)
    
    def forward(self, essay_embeddings, prompt_embeddings, essay_mask=None, prompt_mask=None):
        batch_size = essay_embeddings.size(0)
        
        # ä½¿ç”¨å¹³å‡æ± åŒ–è€Œä¸æ˜¯åŠ æƒå¹³å‡ï¼Œæ›´ç¨³å®š
        if essay_mask is not None:
            # åº”ç”¨maskçš„æ± åŒ–
            essay_masked = essay_embeddings * essay_mask.unsqueeze(-1)
            essay_pooled = essay_masked.sum(dim=1) / essay_mask.sum(dim=1, keepdim=True).clamp(min=1)
        else:
            essay_pooled = essay_embeddings.mean(dim=1)
            
        if prompt_mask is not None:
            prompt_masked = prompt_embeddings * prompt_mask.unsqueeze(-1)
            prompt_pooled = prompt_masked.sum(dim=1) / prompt_mask.sum(dim=1, keepdim=True).clamp(min=1)
        else:
            prompt_pooled = prompt_embeddings.mean(dim=1)
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ä½œä¸ºåŸºç¡€
        cosine_sim = F.cosine_similarity(essay_pooled, prompt_pooled, dim=1).unsqueeze(1)
        
        # ç»„åˆç‰¹å¾
        combined = torch.cat([essay_pooled, prompt_pooled], dim=1)
        
        # è®¡ç®—ç›¸å…³åº¦åˆ†æ•° - åŸºäºç»„åˆç‰¹å¾å’Œä½™å¼¦ç›¸ä¼¼åº¦
        base_relevance = self.similarity_net(combined).squeeze(-1)
        
        # ç»“åˆä½™å¼¦ç›¸ä¼¼åº¦ï¼Œé¿å…ç½‘ç»œé¥±å’Œ
        relevance_score = 0.3 * base_relevance + 0.7 * cosine_sim.squeeze(-1)
        
        return relevance_score, None

class RobustFeatureWeightingModule(nn.Module):
    """ç¨³å¥çš„ç‰¹å¾åŠ æƒæ¨¡å— - ä¸è®­ç»ƒä»£ç å®Œå…¨ä¸€è‡´"""
    def __init__(self, num_features, hidden_dim=64):
        super().__init__()
        self.attention = nn.Sequential(
            nn.Linear(num_features, hidden_dim),
            nn.ReLU(),
            nn.Dropout(DROPOUT),
            nn.Linear(hidden_dim, num_features),
            nn.Sigmoid()
        )
        
        # åˆå§‹åŒ–
        self._init_weights()
        
    def _init_weights(self):
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0.5)  # åˆå§‹åå‘1.0
    
    def forward(self, features):
        weights = self.attention(features)
        weighted_features = features * weights
        return weighted_features, weights

class GraphEncoder(nn.Module):
    """å›¾ç¼–ç å™¨ - ä¸è®­ç»ƒä»£ç å®Œå…¨ä¸€è‡´"""
    def __init__(self, in_dim=GRAPH_IN_DIM, hid=GRAPH_HIDDEN, out_dim=GRAPH_OUT, dropout=DROPOUT):
        super().__init__()
        self.conv1 = GraphConv(in_dim, hid)
        self.conv2 = GraphConv(hid, out_dim)
        self.conv3 = GraphConv(out_dim, out_dim // 2)
        self.norm1 = nn.LayerNorm(hid)
        self.norm2 = nn.LayerNorm(out_dim)
        self.norm3 = nn.LayerNorm(out_dim // 2)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, edge_index, batch):
        x = F.relu(self.norm1(self.conv1(x, edge_index)))
        x = F.relu(self.norm2(self.conv2(x, edge_index)))
        x = self.dropout(F.relu(self.norm3(self.conv3(x, edge_index))))
        g = global_mean_pool(x, batch)
        return g

class FusionGraphBertAES(nn.Module):
    def __init__(self, model_name=MODEL_NAME, graph_out=GRAPH_OUT, 
                 num_features=NUM_FEATURES, hidden_dim=512, dropout=DROPOUT,
                 prompt_relevance_weight=PROMPT_RELEVANCE_WEIGHT, 
                 vocab_size=None, fusion_input_dim=None):
        super().__init__()
        
        self.prompt_relevance_weight = prompt_relevance_weight
        
        # BERTç¼–ç å™¨ - ä½¿ç”¨è®­ç»ƒæ—¶çš„æ¨¡å‹åç§°
        print(f"ğŸ”§ åŠ è½½BERTæ¨¡å‹: {model_name}")
        try:
            # å°è¯•ä½¿ç”¨DebertaV2Tokenizerçš„æ–¹å¼åŠ è½½
            from transformers import DebertaV2Tokenizer
            self.tokenizer = DebertaV2Tokenizer.from_pretrained(model_name)
            print("âœ“ DebertaV2Tokenizer (slow) loaded successfully")
        except Exception as e:
            print(f"DebertaV2Tokenizer failed: {e}")
            try:
                # å¦‚æœå¤±è´¥ï¼Œå°è¯•ä½¿ç”¨AutoTokenizerä½†ç¦ç”¨fastç‰ˆæœ¬
                self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)
                print("âœ“ AutoTokenizer with use_fast=False loaded")
            except Exception as e2:
                print(f"All tokenizer attempts failed: {e2}")
                # æœ€åå›é€€åˆ°å…¶ä»–æ¨¡å‹
                print("Falling back to roberta-base...")
                model_name = "roberta-base"
                self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        
        self.bert = AutoModel.from_pretrained(model_name)
        
        # å¦‚æœæä¾›äº†è¯æ±‡è¡¨å¤§å°ï¼Œè°ƒæ•´embeddingå±‚
        if vocab_size is not None and vocab_size != self.bert.embeddings.word_embeddings.weight.shape[0]:
            print(f"ğŸ”„ è°ƒæ•´è¯æ±‡è¡¨å¤§å°: {self.bert.embeddings.word_embeddings.weight.shape[0]} -> {vocab_size}")
            old_embedding = self.bert.embeddings.word_embeddings
            new_embedding = nn.Embedding(vocab_size, old_embedding.embedding_dim)
            
            # å¤åˆ¶å·²æœ‰çš„æƒé‡ï¼Œæ–°çš„tokenç”¨éšæœºåˆå§‹åŒ–
            min_size = min(vocab_size, old_embedding.weight.shape[0])
            new_embedding.weight.data[:min_size] = old_embedding.weight.data[:min_size]
            
            self.bert.embeddings.word_embeddings = new_embedding
        
        bert_hidden = self.bert.config.hidden_size
        print(f"ğŸ“Š BERTéšè—å±‚ç»´åº¦: {bert_hidden}")
        
        # å†»ç»“BERTå±‚ - ä¸è®­ç»ƒä»£ç å®Œå…¨ä¸€è‡´
        if FREEZE_BERT_LAYERS > 0:
            print(f"ğŸ”’ å†»ç»“å‰ {FREEZE_BERT_LAYERS} å±‚BERTå‚æ•°")
            for i, layer in enumerate(self.bert.encoder.layer):
                if i < FREEZE_BERT_LAYERS:
                    for param in layer.parameters():
                        param.requires_grad = False
        
        # å›¾ç¼–ç å™¨ - ä¸è®­ç»ƒä»£ç å®Œå…¨ä¸€è‡´
        self.graph_encoder = GraphEncoder(in_dim=GRAPH_IN_DIM, hid=GRAPH_HIDDEN, 
                                        out_dim=graph_out, dropout=dropout)
        
        # ç‰¹å¾åŠ æƒæ¨¡å— - ä½¿ç”¨ç¨³å¥ç‰ˆæœ¬
        self.feature_weighting = RobustFeatureWeightingModule(num_features)
        
        # Promptç›¸å…³åº¦æ¨¡å— - ä½¿ç”¨ä¿®å¤ç‰ˆæœ¬
        self.prompt_relevance = FixedPromptRelevanceModule(hidden_dim=bert_hidden, dropout=dropout)
        
        # èåˆå±‚
        if fusion_input_dim is None:
            # è®¡ç®—èåˆè¾“å…¥ç»´åº¦ï¼šBERTéšè—å±‚ + å›¾ç¼–ç å™¨è¾“å‡º + ç‰¹å¾æ•°
            # å›¾ç¼–ç å™¨è¾“å‡ºæ˜¯ GRAPH_OUT // 2ï¼Œå› ä¸ºç¬¬ä¸‰å±‚è¾“å‡ºç»´åº¦å‡åŠ
            graph_output_dim = GRAPH_OUT // 2
            self.fusion_input_dim = bert_hidden + graph_output_dim + num_features
        else:
            self.fusion_input_dim = fusion_input_dim
            
        print(f"ğŸ”§ èåˆè¾“å…¥ç»´åº¦: {self.fusion_input_dim} (BERT:{bert_hidden} + å›¾:{GRAPH_OUT//2} + ç‰¹å¾:{num_features})")
        
        self.fusion_layers = nn.Sequential(
            nn.Linear(self.fusion_input_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, 1)
        )

    def forward(self, input_ids, attention_mask, graph: Batch, features, prompt_ids, prompt_mask):
        # BERTç¼–ç  - ä½œæ–‡
        bert_out = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        essay_embeddings = bert_out.last_hidden_state  # [batch_size, seq_len, hidden_dim]
        bert_pooled = essay_embeddings[:, 0, :]  # ä½¿ç”¨[CLS] token
        
        # BERTç¼–ç  - Prompt (éœ€è¦æ¢¯åº¦ï¼Œå› ä¸ºç›¸å…³åº¦æ¨¡å—éœ€è¦è®­ç»ƒ) - ä¸è®­ç»ƒä»£ç å®Œå…¨ä¸€è‡´
        prompt_out = self.bert(input_ids=prompt_ids, attention_mask=prompt_mask)
        prompt_embeddings = prompt_out.last_hidden_state  # [batch_size, prompt_len, hidden_dim]
        
        # å›¾ç¼–ç 
        graph_emb = self.graph_encoder(graph.x, graph.edge_index, graph.batch)
        
        # ç‰¹å¾åŠ æƒ
        weighted_features, feature_weights = self.feature_weighting(features)
        
        # Promptç›¸å…³åº¦è®¡ç®— - ä¿®å¤ç‰ˆæœ¬
        relevance_score, _ = self.prompt_relevance(
            essay_embeddings, prompt_embeddings, attention_mask, prompt_mask
        )
        
        # ç‰¹å¾èåˆ
        x = torch.cat([bert_pooled, graph_emb, weighted_features], dim=1)
        
        # æ£€æŸ¥ç»´åº¦æ˜¯å¦åŒ¹é…
        if x.shape[1] != self.fusion_input_dim:
            print(f"âš  ç»´åº¦ä¸åŒ¹é…: æœŸæœ› {self.fusion_input_dim}, å®é™… {x.shape[1]}")
            print(f"   å„ç»„ä»¶ç»´åº¦: BERT({bert_pooled.shape}), Graph({graph_emb.shape}), Features({weighted_features.shape})")
            raise ValueError(f"èåˆå±‚ç»´åº¦ä¸åŒ¹é…: æœŸæœ› {self.fusion_input_dim}, å®é™… {x.shape[1]}")
        
        # åŸºç¡€è¯„åˆ†
        base_score = self.fusion_layers(x).squeeze(-1)
        
        # ç»“åˆPromptç›¸å…³åº¦è°ƒæ•´æœ€ç»ˆè¯„åˆ† - ä¸è®­ç»ƒä»£ç å®Œå…¨ä¸€è‡´
        # ç›¸å…³åº¦åˆ†æ•°ä½œä¸ºç½®ä¿¡åº¦æƒé‡
        relevance_weight = 1 + (relevance_score - 0.5) * self.prompt_relevance_weight * 50
        final_score = base_score * relevance_weight
        
        # ç¡®ä¿è¯„åˆ†åœ¨åˆç†èŒƒå›´å†…
        final_score = torch.clamp(final_score, 0.0, 1.0)
        
        # è¿”å›æ›´å¤šä¸­é—´ç»“æœç”¨äºåˆ†æ
        intermediate_outputs = {
            'bert_pooled': bert_pooled,
            'graph_embedding': graph_emb,
            'weighted_features': weighted_features,
            'feature_weights': feature_weights,
            'relevance_score': relevance_score,
            'base_score': base_score,
            'relevance_weight': relevance_weight
        }
        
        return final_score, intermediate_outputs

# ---------------------------
# ---- å¢å¼ºçš„è¯„åˆ†å™¨ ----
# ---------------------------
class EssayScorer:
    def __init__(self, model_path="fusion_graphbert_best.pth", 
                 feature_scaler_path="feature_scaler.pkl",
                 target_scaler_path="target_scaler.pkl",
                 config_path="model_config.json",
                 tokenizer_path="./tokenizer",
                 device=None):
        self.device = device if device else torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"Using device: {self.device}")
        
        # é¦–å…ˆåŠ è½½é…ç½®
        self.model_config = self._load_model_config(config_path)
        
        # ç„¶ååŠ è½½ç»„ä»¶ï¼ˆä½¿ç”¨é…ç½®ä¸­çš„å‚æ•°ï¼‰
        self._load_components(tokenizer_path)
        self._load_scalers(feature_scaler_path, target_scaler_path)
        self._load_model(model_path)
        
        print("âœ“ Essay Scorer initialized successfully")
    
    def _load_model_config(self, config_path):
        """åŠ è½½æ¨¡å‹é…ç½®"""
        if os.path.exists(config_path):
            with open(config_path, 'r') as f:
                config = json.load(f)
            print(f"âœ“ Model config loaded from {config_path}")
            
            # ç¡®ä¿é…ç½®ä¸­æœ‰å¿…è¦çš„å­—æ®µ
            if "vocab_size" not in config:
                # ä»tokenizerç›®å½•æ¨æ–­è¯æ±‡è¡¨å¤§å°
                tokenizer_path = "./tokenizer"
                if os.path.exists(tokenizer_path):
                    try:
                        tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)
                        config["vocab_size"] = len(tokenizer)
                        print(f"ğŸ“š ä»tokenizeræ¨æ–­è¯æ±‡è¡¨å¤§å°: {config['vocab_size']}")
                    except:
                        config["vocab_size"] = 50265  # é»˜è®¤å€¼
                        print(f"âš  ä½¿ç”¨é»˜è®¤è¯æ±‡è¡¨å¤§å°: {config['vocab_size']}")
            
            if "fusion_input_dim" not in config:
                # è®¡ç®—èåˆè¾“å…¥ç»´åº¦
                model_name = config.get("MODEL_NAME", "roberta-base")
                try:
                    bert_config = AutoConfig.from_pretrained(model_name)
                    bert_hidden = bert_config.hidden_size
                    # å›¾ç¼–ç å™¨è¾“å‡ºç»´åº¦æ˜¯ GRAPH_OUT // 2
                    graph_out = config.get("GRAPH_OUT", 256) // 2
                    num_features = config.get("NUM_FEATURES", 20)
                    config["fusion_input_dim"] = bert_hidden + graph_out + num_features
                    print(f"ğŸ”§ è®¡ç®—èåˆè¾“å…¥ç»´åº¦: {config['fusion_input_dim']}")
                except:
                    config["fusion_input_dim"] = 768 + 128 + 20  # é»˜è®¤å€¼
                    print(f"âš  ä½¿ç”¨é»˜è®¤èåˆè¾“å…¥ç»´åº¦: {config['fusion_input_dim']}")
            
            return config
        else:
            print(f"âŒ Model config file {config_path} not found.")
            raise FileNotFoundError(f"Model config file {config_path} not found")
    
    def _load_components(self, tokenizer_path):
        """åŠ è½½æ‰€æœ‰å¿…è¦çš„ç»„ä»¶ - ä½¿ç”¨ä¿å­˜çš„tokenizer"""
        # åŠ è½½ä¿å­˜çš„tokenizer
        print("Loading tokenizer...")
        if os.path.exists(tokenizer_path):
            try:
                self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)
                print(f"âœ“ Tokenizer loaded from {tokenizer_path}")
                print(f"ğŸ“š Tokenizer vocab size: {len(self.tokenizer)}")
            except Exception as e:
                print(f"âŒ Error loading tokenizer: {e}")
                raise
        else:
            print(f"âŒ Tokenizer path {tokenizer_path} not found.")
            raise FileNotFoundError(f"Tokenizer path {tokenizer_path} not found")
        
        # åŠ è½½spaCy - ä¸è®­ç»ƒä»£ç å®Œå…¨ä¸€è‡´
        print("Loading spaCy model...")
        try:
            self.nlp = spacy.load("en_core_web_sm", disable=["ner"])
            print("âœ“ spaCy model loaded")
        except OSError:
            print("Downloading spaCy English model...")
            os.system("python -m spacy download en_core_web_sm")
            self.nlp = spacy.load("en_core_web_sm", disable=["ner"])
        
        # åˆå§‹åŒ–ç‰¹å¾æå–å™¨ - ä¸è®­ç»ƒä»£ç å®Œå…¨ä¸€è‡´
        self.feature_extractor = FeatureExtractor()
        
        # ä¸‹è½½nltkæ•°æ® - ä¸è®­ç»ƒä»£ç å®Œå…¨ä¸€è‡´
        try:
            nltk.download('punkt', quiet=True)
            nltk.download('averaged_perceptron_tagger', quiet=True)
        except:
            pass
    
    def _load_scalers(self, feature_scaler_path, target_scaler_path):
        """åŠ è½½ç‰¹å¾æ ‡å‡†åŒ–å™¨å’Œç›®æ ‡åˆ†æ•°å½’ä¸€åŒ–å™¨"""
        # åŠ è½½ç‰¹å¾æ ‡å‡†åŒ–å™¨
        try:
            if os.path.exists(feature_scaler_path):
                with open(feature_scaler_path, 'rb') as f:
                    self.feature_scaler = pickle.load(f)
                print(f"âœ“ Feature scaler loaded from {feature_scaler_path}")
            else:
                print(f"âŒ Feature scaler file {feature_scaler_path} not found.")
                raise FileNotFoundError(f"Feature scaler file {feature_scaler_path} not found")
        except Exception as e:
            print(f"âŒ Error loading feature scaler: {e}")
            raise
        
        # åŠ è½½ç›®æ ‡åˆ†æ•°å½’ä¸€åŒ–å™¨
        try:
            if os.path.exists(target_scaler_path):
                with open(target_scaler_path, 'rb') as f:
                    self.target_scaler = pickle.load(f)
                print(f"âœ“ Target scaler loaded from {target_scaler_path}")
                # æ£€æŸ¥scalerç±»å‹
                print(f"ğŸ“Š Target scaler type: {type(self.target_scaler)}")
            else:
                print(f"âŒ Target scaler file {target_scaler_path} not found.")
                raise FileNotFoundError(f"Target scaler file {target_scaler_path} not found")
        except Exception as e:
            print(f"âŒ Error loading target scaler: {e}")
            raise
    
    def _load_model(self, model_path):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        print("Loading trained model...")
        
        # ä½¿ç”¨é…ç½®ä¸­çš„å‚æ•°åˆ›å»ºæ¨¡å‹
        model_name = self.model_config.get("MODEL_NAME", "roberta-base")
        vocab_size = self.model_config.get("vocab_size", len(self.tokenizer))
        fusion_input_dim = self.model_config.get("fusion_input_dim")
        
        print(f"ğŸ”§ åˆ›å»ºæ¨¡å‹: {model_name}, vocab_size={vocab_size}, fusion_input_dim={fusion_input_dim}")
        
        self.model = FusionGraphBertAES(
            model_name=model_name,
            graph_out=self.model_config.get("GRAPH_OUT", 256),
            num_features=self.model_config.get("NUM_FEATURES", 20),
            dropout=self.model_config.get("DROPOUT", 0.3),
            prompt_relevance_weight=self.model_config.get("PROMPT_RELEVANCE_WEIGHT", 0.15),
            vocab_size=vocab_size,
            fusion_input_dim=fusion_input_dim
        ).to(self.device)
        
        # åŠ è½½è®­ç»ƒå¥½çš„æƒé‡
        if os.path.exists(model_path):
            try:
                state_dict = torch.load(model_path, map_location=self.device)
                # ä¸¥æ ¼åŠ è½½ï¼Œç¡®ä¿æ‰€æœ‰å‚æ•°åŒ¹é…
                self.model.load_state_dict(state_dict, strict=True)
                print(f"âœ“ Model loaded from {model_path}")
            except Exception as e:
                print(f"âŒ Error loading model: {e}")
                # å°è¯•éä¸¥æ ¼åŠ è½½
                try:
                    self.model.load_state_dict(state_dict, strict=False)
                    print("âœ“ Model loaded with strict=False")
                except Exception as e2:
                    print(f"âŒ Failed to load model even with strict=False: {e2}")
                    raise
        else:
            print(f"âŒ Model file {model_path} not found.")
            raise FileNotFoundError(f"Model file {model_path} not found")
        
        self.model.eval()
        print("âœ“ Model set to evaluation mode")
    
    def preprocess_essay(self, essay_text, prompt_text):
        """é¢„å¤„ç†å•ç¯‡ä½œæ–‡ - ä¸è®­ç»ƒä»£ç å®Œå…¨ä¸€è‡´"""
        # BERTç¼–ç ä½œæ–‡
        essay_enc = self.tokenizer(
            essay_text, 
            truncation=True, 
            padding='max_length', 
            max_length=MAX_LENGTH, 
            return_tensors='pt'
        )
        
        # BERTç¼–ç Prompt
        prompt_enc = self.tokenizer(
            prompt_text, 
            truncation=True, 
            padding='max_length', 
            max_length=128, 
            return_tensors='pt'
        )
        
        # æ„å»ºå›¾
        graph, graph_metrics = build_graph_from_text(essay_text, self.nlp)
        
        # æå–ç‰¹å¾å¹¶æ ‡å‡†åŒ–
        features, feature_details = self.feature_extractor.extract_features(essay_text)
        features_scaled = self.feature_scaler.transform(features.reshape(1, -1)).squeeze()
        features_tensor = torch.tensor(features_scaled, dtype=torch.float32)
        
        return {
            "input_ids": essay_enc['input_ids'].squeeze(0),
            "attention_mask": essay_enc['attention_mask'].squeeze(0),
            "graph": graph,
            "graph_metrics": graph_metrics,
            "features": features_tensor,
            "features_raw": features,
            "feature_details": feature_details,
            "prompt_ids": prompt_enc['input_ids'].squeeze(0),
            "prompt_mask": prompt_enc['attention_mask'].squeeze(0)
        }
    
    def _denormalize_score(self, normalized_score):
        """åå½’ä¸€åŒ–åˆ†æ•° - ä¿®å¤äº†GradScaleré”™è¯¯"""
        try:
            # ç¡®ä¿è¾“å…¥æ˜¯æ ‡é‡å€¼ï¼Œè€Œä¸æ˜¯å¼ é‡
            if torch.is_tensor(normalized_score):
                normalized_score = normalized_score.item()
            
            # æ£€æŸ¥target_scalerç±»å‹
            if hasattr(self.target_scaler, 'inverse_transform'):
                # å¦‚æœæ˜¯sklearnçš„scaler
                score_np = np.array([[normalized_score]])
                denormalized = self.target_scaler.inverse_transform(score_np)[0][0]
                return float(denormalized)
            else:
                # é»˜è®¤ä½¿ç”¨MinMaxå½’ä¸€åŒ–
                min_score = self.model_config.get("min_score", 1.0)
                max_score = self.model_config.get("max_score", 5.0)
                denormalized = normalized_score * (max_score - min_score) + min_score
                return float(denormalized)
        except Exception as e:
            print(f"âš  åå½’ä¸€åŒ–å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤æ–¹æ³•")
            min_score = self.model_config.get("min_score", 1.0)
            max_score = self.model_config.get("max_score", 5.0)
            denormalized = normalized_score * (max_score - min_score) + min_score
            return float(denormalized)
    
    def _analyze_model_outputs(self, intermediate_outputs):
        """åˆ†ææ¨¡å‹çš„ä¸­é—´è¾“å‡ºï¼Œæå–å¤šç»´ç‰¹å¾æŒ‡æ ‡"""
        analysis = {}
        
        # BERT embedding åˆ†æ
        bert_pooled = intermediate_outputs['bert_pooled']
        analysis['bert_embedding_norm'] = float(torch.norm(bert_pooled).item())
        analysis['bert_embedding_mean'] = float(torch.mean(bert_pooled).item())
        analysis['bert_embedding_std'] = float(torch.std(bert_pooled).item())
        
        # Graph embedding åˆ†æ
        graph_emb = intermediate_outputs['graph_embedding']
        analysis['graph_embedding_norm'] = float(torch.norm(graph_emb).item())
        analysis['graph_embedding_mean'] = float(torch.mean(graph_emb).item())
        
        # ç‰¹å¾æƒé‡åˆ†æ
        feature_weights = intermediate_outputs['feature_weights']
        analysis['feature_weights_mean'] = float(torch.mean(feature_weights).item())
        analysis['feature_weights_std'] = float(torch.std(feature_weights).item())
        analysis['feature_weights_max'] = float(torch.max(feature_weights).item())
        analysis['feature_weights_min'] = float(torch.min(feature_weights).item())
        
        # ç›¸å…³åº¦åˆ†æ
        analysis['relevance_score'] = float(intermediate_outputs['relevance_score'].item())
        analysis['relevance_weight'] = float(intermediate_outputs['relevance_weight'].item())
        
        return analysis
    
    def score_single_essay(self, essay_text, prompt_text, return_details=True):
        """å¯¹å•ç¯‡ä½œæ–‡è¿›è¡Œè¯„åˆ†ï¼Œè¿”å›å¤šç»´ç‰¹å¾æŒ‡æ ‡"""
        try:
            print(f"ğŸ“ å¼€å§‹è¯„åˆ†ä½œæ–‡ (è¯æ•°: {len(essay_text.split())})")
            
            # é¢„å¤„ç†
            data = self.preprocess_essay(essay_text, prompt_text)
            
            # å‡†å¤‡æ‰¹é‡æ•°æ®
            input_ids = data["input_ids"].unsqueeze(0).to(self.device)
            attention_mask = data["attention_mask"].unsqueeze(0).to(self.device)
            graph_batch = Batch.from_data_list([data["graph"]]).to(self.device)
            features = data["features"].unsqueeze(0).to(self.device)
            prompt_ids = data["prompt_ids"].unsqueeze(0).to(self.device)
            prompt_mask = data["prompt_mask"].unsqueeze(0).to(self.device)
            
            print("ğŸ§  è¿è¡Œæ¨¡å‹æ¨ç†...")
            # æ¨¡å‹é¢„æµ‹
            with torch.no_grad():
                final_score, intermediate_outputs = self.model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    graph=graph_batch,
                    features=features,
                    prompt_ids=prompt_ids,
                    prompt_mask=prompt_mask
                )
            
            # ä½¿ç”¨ä¿®å¤çš„åå½’ä¸€åŒ–æ–¹æ³•
            final_score_denorm = self._denormalize_score(final_score)
            base_score_denorm = self._denormalize_score(intermediate_outputs['base_score'])
            
            # åˆ†ææ¨¡å‹ä¸­é—´è¾“å‡º
            model_analysis = self._analyze_model_outputs(intermediate_outputs)
            
            # æ„å»ºè¯¦ç»†ç»“æœ
            score_details = {
                "final_score": round(final_score_denorm, 2),
                "base_score": round(base_score_denorm, 2),
                "prompt_relevance": round(model_analysis['relevance_score'], 3),
                "adjusted_by_relevance": round((final_score_denorm - base_score_denorm), 2),
                "word_count": len(essay_text.split()),
                "char_count": len(essay_text),
                
                # æ¨¡å‹å†…éƒ¨ç‰¹å¾
                "model_features": {
                    "bert_embedding": {
                        "norm": round(model_analysis['bert_embedding_norm'], 4),
                        "mean": round(model_analysis['bert_embedding_mean'], 4),
                        "std": round(model_analysis['bert_embedding_std'], 4)
                    },
                    "graph_embedding": {
                        "norm": round(model_analysis['graph_embedding_norm'], 4),
                        "mean": round(model_analysis['graph_embedding_mean'], 4)
                    },
                    "feature_weights": {
                        "mean": round(model_analysis['feature_weights_mean'], 4),
                        "std": round(model_analysis['feature_weights_std'], 4),
                        "max": round(model_analysis['feature_weights_max'], 4),
                        "min": round(model_analysis['feature_weights_min'], 4)
                    },
                    "relevance_metrics": {
                        "score": round(model_analysis['relevance_score'], 4),
                        "weight": round(model_analysis['relevance_weight'], 4)
                    }
                },
                
                # è¯­è¨€ç‰¹å¾
                "linguistic_features": data["feature_details"],
                
                # å›¾ç»“æ„ç‰¹å¾
                "graph_structure": data["graph_metrics"],
                
                # åŸå§‹ç‰¹å¾å€¼ï¼ˆæ ‡å‡†åŒ–å‰ï¼‰
                "raw_features": dict(zip(self.feature_extractor.feature_names, 
                                       [round(x, 2) for x in data["features_raw"]]))
            }
            
            print(f"âœ… è¯„åˆ†å®Œæˆ: {final_score_denorm:.2f}")
            
            return score_details
                
        except Exception as e:
            print(f"âŒ Error scoring essay: {e}")
            import traceback
            traceback.print_exc()
            
            error_result = {
                "final_score": None,
                "base_score": None,
                "prompt_relevance": None,
                "adjusted_by_relevance": None,
                "word_count": len(essay_text.split()),
                "char_count": len(essay_text),
                "error": str(e),
                "model_features": None,
                "linguistic_features": None,
                "graph_structure": None,
                "raw_features": None
            }
            return error_result
    
    def score_multiple_essays(self, essays_data):
        """
        å¯¹å¤šç¯‡ä½œæ–‡è¿›è¡Œæ‰¹é‡è¯„åˆ†
        
        Args:
            essays_data: list of dict, æ¯ä¸ªdictåŒ…å«:
                - 'essay_text': ä½œæ–‡æ–‡æœ¬
                - 'prompt_text': é¢˜ç›®æ–‡æœ¬
                - 'essay_id': å¯é€‰ï¼Œä½œæ–‡ID
        
        Returns:
            list of dict: è¯„åˆ†ç»“æœ
        """
        results = []
        
        for i, essay_data in enumerate(essays_data):
            essay_text = essay_data['essay_text']
            prompt_text = essay_data['prompt_text']
            essay_id = essay_data.get('essay_id', f"essay_{i+1}")
            
            print(f"\n{'='*50}")
            print(f"è¯„åˆ†ä½œæ–‡ {essay_id}")
            print(f"{'='*50}")
            
            score_details = self.score_single_essay(essay_text, prompt_text)
            
            result = {
                'essay_id': essay_id,
                'final_score': score_details['final_score'],
                'base_score': score_details['base_score'],
                'prompt_relevance': score_details['prompt_relevance'],
                'adjusted_by_relevance': score_details['adjusted_by_relevance'],
                'word_count': score_details['word_count'],
                'char_count': score_details['char_count'],
                'model_features': score_details['model_features'],
                'linguistic_features': score_details['linguistic_features'],
                'graph_structure': score_details['graph_structure'],
                'raw_features': score_details['raw_features']
            }
            
            if 'error' in score_details:
                result['error'] = score_details['error']
            
            results.append(result)
            
            if score_details['final_score'] is not None:
                print(f"âœ… {essay_id}: æœ€ç»ˆåˆ†æ•° {score_details['final_score']} | ç›¸å…³åº¦ {score_details['prompt_relevance']}")
                print(f"   BERTåµŒå…¥èŒƒæ•°: {score_details['model_features']['bert_embedding']['norm']}")
                print(f"   å›¾åµŒå…¥èŒƒæ•°: {score_details['model_features']['graph_embedding']['norm']}")
                print(f"   ç‰¹å¾æƒé‡å‡å€¼: {score_details['model_features']['feature_weights']['mean']}")
            else:
                print(f"âŒ {essay_id}: è¯„åˆ†å¤±è´¥ - {score_details.get('error', 'Unknown error')}")
        
        return results

# ---------------------------
# ---- ä½¿ç”¨ç¤ºä¾‹ ----
# ---------------------------
def main():
    try:
        # åˆå§‹åŒ–è¯„åˆ†å™¨
        scorer = EssayScorer(
            model_path="fusion_graphbert_best.pth",
            feature_scaler_path="feature_scaler.pkl",
            target_scaler_path="target_scaler.pkl",
            config_path="model_config.json",
            tokenizer_path="./tokenizer"
        )
        
        # æµ‹è¯•è¯„åˆ†
        prompt_text = """Environmental Protection: A Shared Responsibility"""
        essay_text = """

In the modern era, environmental protection has emerged as an increasingly urgent global issue that demands our immediate and collective attention. The Earth, our only home in the vast universe, is facing a multitude of environmental challenges that pose a grave threat to the very existence of life as we know it.

One of the most pressing problems is air pollution. Factories, with their towering chimneys, emit large amounts of harmful gases such as sulfur dioxide and nitrogen oxides into the atmosphere. Meanwhile, the ever - increasing number of vehicles on the road continuously releases exhaust fumes. These pollutants not only cause a range of respiratory diseases in humans, including asthma and lung cancer, but also lead to the formation of smog. Smog blankets cities, reducing visibility and disrupting daily life, from traffic congestion to flight delays.

Water pollution is another significant concern that cannot be ignored. Industrial waste, often containing toxic chemicals, agricultural runoff laden with pesticides and fertilizers, and untreated sewage are frequently discharged into rivers and oceans. This contamination kills a vast number of aquatic organisms, disrupts the balance of marine ecosystems, and makes the water unfit for human consumption and recreational activities.

To address these pressing issues, we all have a crucial role to play. Governments should take the lead by enacting and strictly enforcing comprehensive environmental laws and regulations. They can impose limits on pollution emissions from industries and vehicles, and offer incentives for companies to adopt cleaner production technologies. Additionally, investing in renewable energy sources like solar and wind power is essential to reduce our reliance on fossil fuels, which are major contributors to environmental degradation.

As individuals, we can make simple yet effective changes in our daily lives. We can practice the three Rs - reduce, reuse, and recycle - to minimize waste generation. Choosing to walk, bike, or use public transportation instead of driving alone can significantly cut down on air pollution. Moreover, conserving water by fixing leaks and using it wisely is of great importance.

In conclusion, environmental protection is not a one - person job but a shared responsibility that transcends national boundaries. Only by working together at all levels, from governments to individuals, can we safeguard our planet and ensure a sustainable and healthy future for generations to come.
      """
        
        score_details = scorer.score_single_essay(essay_text, prompt_text)
        
        print("\nğŸ“Š è¯¦ç»†è¯„åˆ†ç»“æœ:")
        print(f"\nğŸ¯ è¯„åˆ†æŒ‡æ ‡:")
        print(f"  æœ€ç»ˆåˆ†æ•°: {score_details['final_score']}")
        print(f"  åŸºç¡€åˆ†æ•°: {score_details['base_score']}")
        print(f"  Promptç›¸å…³åº¦: {score_details['prompt_relevance']}")
        print(f"  ç›¸å…³åº¦è°ƒæ•´: {score_details['adjusted_by_relevance']}")
        
        print(f"\nğŸ“ æ–‡æœ¬ç»Ÿè®¡:")
        print(f"  è¯æ•°: {score_details['word_count']}")
        print(f"  å­—ç¬¦æ•°: {score_details['char_count']}")
        
        print(f"\nğŸ§  æ¨¡å‹å†…éƒ¨ç‰¹å¾:")
        mf = score_details['model_features']
        print(f"  BERTåµŒå…¥ - èŒƒæ•°: {mf['bert_embedding']['norm']}, å‡å€¼: {mf['bert_embedding']['mean']}, æ ‡å‡†å·®: {mf['bert_embedding']['std']}")
        print(f"  å›¾åµŒå…¥ - èŒƒæ•°: {mf['graph_embedding']['norm']}, å‡å€¼: {mf['graph_embedding']['mean']}")
        print(f"  ç‰¹å¾æƒé‡ - å‡å€¼: {mf['feature_weights']['mean']}, æ ‡å‡†å·®: {mf['feature_weights']['std']}")
        print(f"  ç›¸å…³åº¦æŒ‡æ ‡ - åˆ†æ•°: {mf['relevance_metrics']['score']}, æƒé‡: {mf['relevance_metrics']['weight']}")
        
        print(f"\nğŸ“ˆ è¯­è¨€ç‰¹å¾ (å‰5ä¸ªæœ€é‡è¦çš„):")
        linguistic = score_details['linguistic_features']
        # æŒ‰å€¼æ’åºæ˜¾ç¤ºæœ€é‡è¦çš„ç‰¹å¾
        sorted_features = sorted(linguistic.items(), key=lambda x: abs(x[1]), reverse=True)[:5]
        for name, value in sorted_features:
            print(f"  {name}: {value:.2f}")
        
        print(f"\nğŸ•¸ï¸ å›¾ç»“æ„ç‰¹å¾:")
        graph = score_details['graph_structure']
        for key, value in graph.items():
            print(f"  {key}: {value:.2f}")
        
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()