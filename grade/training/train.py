# ================================================================
# FusionGraphBERT-AES: ç»“åˆå›¾ç¥ç»ç½‘ç»œä¸ç‰¹å¾å·¥ç¨‹çš„è‡ªåŠ¨ä½œæ–‡è¯„åˆ†ç³»ç»Ÿ
# ================================================================

import os
import hashlib
import json
import random
import pickle
from collections import Counter
from datetime import datetime

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.optim import AdamW
from torch_geometric.data import Data, Batch
from torch_geometric.nn import GraphConv, global_mean_pool
from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler

import transformers
from transformers import AutoTokenizer, AutoModel
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.metrics import cohen_kappa_score, confusion_matrix, classification_report, mean_squared_error, mean_absolute_error
from tqdm import tqdm
import spacy
import matplotlib.pyplot as plt
import seaborn as sns
import nltk
import re
from textstat.textstat import textstat

# ---------------------------
# -------- CONFIG -----------
# ---------------------------
DATASET_PATH = "./ASAP2_train_sourcetexts.csv"
GRAPH_CACHE_DIR = "graphs_cache"
FEATURE_CACHE_DIR = "features_cache"
PROMPT_CACHE_DIR = "prompt_cache"
os.makedirs(GRAPH_CACHE_DIR, exist_ok=True)
os.makedirs(FEATURE_CACHE_DIR, exist_ok=True)
os.makedirs(PROMPT_CACHE_DIR, exist_ok=True)

# æ¨¡å‹é…ç½®
MODEL_NAME = "microsoft/deberta-v3-base"
# ä¿®æ”¹tokenizeråŠ è½½æ–¹å¼
try:
    # é¦–å…ˆå°è¯•ä½¿ç”¨æ…¢é€Ÿtokenizer
    from transformers import DebertaV2Tokenizer
    tokenizer = DebertaV2Tokenizer.from_pretrained(MODEL_NAME)
    print("âœ“ DebertaV2Tokenizer (slow) loaded successfully")
except Exception as e:
    print(f"DebertaV2Tokenizer failed: {e}")
    try:
        # å¦‚æœå¤±è´¥ï¼Œå°è¯•ä½¿ç”¨AutoTokenizerä½†ç¦ç”¨fastç‰ˆæœ¬
        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)
        print("âœ“ AutoTokenizer with use_fast=False loaded")
    except Exception as e2:
        print(f"All tokenizer attempts failed: {e2}")
        # æœ€åå›é€€åˆ°å…¶ä»–æ¨¡å‹
        print("Falling back to roberta-base...")
        MODEL_NAME = "roberta-base"
        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
MAX_LENGTH = 256
BATCH_SIZE = 8
EPOCHS = 20
LR_BERT = 5e-6
LR_GRAPH = 1e-4
LR_FEATURES = 1e-3
DROPOUT = 0.3
PATIENCE = 5
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
ACCUM_STEPS = 2
FREEZE_BERT_LAYERS = 6

# å›¾ç¼–ç å™¨é…ç½®
GRAPH_IN_DIM = 2
GRAPH_HIDDEN = 64
GRAPH_OUT = 256

# ç‰¹å¾å·¥ç¨‹é…ç½®
NUM_FEATURES = 20

# Promptç›¸å…³åº¦é…ç½®
PROMPT_RELEVANCE_WEIGHT = 0.15

print("Using device:", DEVICE)

# ---------------------------
# ---- ç‰¹å¾å·¥ç¨‹å‡½æ•° ----
# ---------------------------
class FeatureExtractor:
    def __init__(self):
        self.dale_chall_common_words = self._load_dale_chall_words()
        try:
            nltk.download('punkt', quiet=True)
            nltk.download('averaged_perceptron_tagger', quiet=True)
        except:
            pass
    
    def _load_dale_chall_words(self):
        """åŠ è½½å¸¸ç”¨è¯è¡¨"""
        dale_chall_url = "https://gist.githubusercontent.com/Abhishek-P/e00edcc6f508640fe24f263f5836a7dc/raw/166225e09fb8b554deff37ec344ad5ca40dab2fb/dale-chall-3000-words.txt"
        try:
            import requests
            response = requests.get(dale_chall_url, timeout=10)
            if response.status_code == 200:
                return set(response.text.splitlines())
        except:
            pass
        return set()
    
    def safe_word_tokenize(self, text):
        """å®‰å…¨çš„åˆ†è¯å‡½æ•°"""
        try:
            return nltk.word_tokenize(str(text))
        except:
            try:
                return str(text).split()
            except:
                return []
    
    def extract_features(self, text):
        """æå–20ç»´è¯­è¨€å­¦ç‰¹å¾"""
        if pd.isna(text) or text == "" or text is None:
            return np.zeros(NUM_FEATURES, dtype=float)
        
        try:
            text_str = str(text)
            tokens = self.safe_word_tokenize(text_str)
            words = [t for t in tokens if re.match(r"\w+", t)]
            n_chars = len(text_str)
            n_words = len(words)
            n_sent = max(1, text_str.count('.') + text_str.count('!') + text_str.count('?'))
            uniq = len(set(w.lower() for w in words))
            
            # è¯æ€§æ ‡æ³¨
            try:
                pos = nltk.pos_tag(words)
                pos_counts = Counter(tag for _, tag in pos)
            except:
                pos_counts = Counter()

            # åŸºç¡€ç‰¹å¾
            ch = n_chars
            w = n_words
            co = text_str.count(',')
            uw = uniq

            # è¯æ€§ç‰¹å¾
            nnp = pos_counts.get('NNP', 0)
            dt = pos_counts.get('DT', 0)
            nn = pos_counts.get('NN', 0)
            rb = pos_counts.get('RB', 0)
            jj = pos_counts.get('JJ', 0)
            inn = pos_counts.get('IN', 0)

            # å¯è¯»æ€§ç‰¹å¾
            try:
                fog = textstat.gunning_fog(text_str) if n_words > 0 else 0
            except:
                fog = 0
            try:
                smog = textstat.smog_index(text_str) if n_words > 0 else 0
            except:
                smog = 0
            try:
                rix = textstat.rix(text_str) if n_words > 0 else 0
            except:
                rix = 0
            try:
                dc = textstat.dale_chall_readability_score(text_str) if n_words > 0 else 0
            except:
                dc = 0
                
            wt = len(set(words))
            s = n_sent
            lw = sum(1 for w in words if len(w) > 6)
            
            try:
                cw = sum(1 for w in words if textstat.syllable_count(w) > 2)
            except:
                cw = 0
                
            nbw = sum(1 for w in words if w.lower() not in self.dale_chall_common_words)
            
            try:
                dw = sum(1 for w in words if len(textstat.difficult_words_list([w])) > 0)
            except:
                dw = 0

            feats = [ch, w, co, uw, nnp, dt, nn, rb, jj, inn, fog, smog, rix, dc, wt, s, lw, cw, nbw, dw]
            return np.array(feats, dtype=float)
            
        except Exception as e:
            print(f"Error computing features: {e}")
            return np.zeros(NUM_FEATURES, dtype=float)

feature_extractor = FeatureExtractor()

# ---------------------------
# ---- å›¾æ„å»ºå’Œç¼“å­˜ ----
# ---------------------------
print("Loading spaCy model...")
nlp = spacy.load("en_core_web_sm", disable=["ner"])

def text_hash(text: str) -> str:
    return hashlib.md5(text.encode("utf-8")).hexdigest()

def build_graph_from_text(text: str, max_nodes: int = 256) -> Data:
    """æ„å»ºä¾èµ–å›¾"""
    doc = nlp(text)
    nodes = list(doc)[:max_nodes]
    if len(nodes) == 0:
        x = torch.zeros((1, GRAPH_IN_DIM), dtype=torch.float32)
        edge_index = torch.tensor([[0],[0]], dtype=torch.long)
        return Data(x=x, edge_index=edge_index)

    edges = []
    for token in nodes:
        head_idx = token.head.i
        if token.i != head_idx and head_idx < max_nodes:
            edges.append((token.i, head_idx))
    if len(edges) == 0:
        edges = [(0, 0)]

    pos_ids = [int(tok.pos) for tok in nodes]
    is_root = [1 if tok.dep_ == "ROOT" else 0 for tok in nodes]
    x = torch.tensor(list(zip(pos_ids, is_root)), dtype=torch.float32)
    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()
    return Data(x=x, edge_index=edge_index)

def get_cached_graph(text: str) -> Data:
    h = text_hash(text)
    path = os.path.join(GRAPH_CACHE_DIR, f"{h}.pt")
    if os.path.exists(path):
        try:
            return torch.load(path, weights_only=False)
        except:
            pass
    g = build_graph_from_text(text, max_nodes=256)
    torch.save(g, path)
    return g

def get_cached_features(text: str) -> np.ndarray:
    h = text_hash(text)
    path = os.path.join(FEATURE_CACHE_DIR, f"{h}.npy")
    if os.path.exists(path):
        try:
            return np.load(path)
        except:
            pass
    features = feature_extractor.extract_features(text)
    np.save(path, features)
    return features

# ---------------------------
# ---- Promptç›¸å…³åº¦æ¨¡å— ----
# ---------------------------
class FixedPromptRelevanceModule(nn.Module):
    """ä¿®å¤çš„Promptç›¸å…³åº¦è¯„ä¼°æ¨¡å— - è§£å†³è¾“å‡ºé¥±å’Œé—®é¢˜"""
    def __init__(self, hidden_dim=128, dropout=0.1):
        super().__init__()
        self.hidden_dim = hidden_dim
        
        # ä½¿ç”¨æ›´ç®€å•çš„æ¶æ„ï¼Œé¿å…é¥±å’Œ
        self.essay_pool = nn.AdaptiveAvgPool1d(1)
        self.prompt_pool = nn.AdaptiveAvgPool1d(1)
        
        # ç›¸ä¼¼åº¦è®¡ç®— - ä½¿ç”¨æ›´å°çš„ç½‘ç»œ
        self.similarity_net = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim // 4),
            nn.Tanh(),  # ä½¿ç”¨Tanhé¿å…é¥±å’Œ
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 4, hidden_dim // 8),
            nn.Tanh(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 8, 1),
            nn.Sigmoid()
        )
        
        # æ›´ä¿å®ˆçš„åˆå§‹åŒ–
        self._init_weights()
        
    def _init_weights(self):
        for module in self.modules():
            if isinstance(module, nn.Linear):
                # ä½¿ç”¨æ›´å°çš„åˆå§‹åŒ–æƒé‡
                nn.init.normal_(module.weight, mean=0.0, std=0.02)
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0.0)
    
    def forward(self, essay_embeddings, prompt_embeddings, essay_mask=None, prompt_mask=None):
        batch_size = essay_embeddings.size(0)
        
        # ä½¿ç”¨å¹³å‡æ± åŒ–è€Œä¸æ˜¯åŠ æƒå¹³å‡ï¼Œæ›´ç¨³å®š
        if essay_mask is not None:
            # åº”ç”¨maskçš„æ± åŒ–
            essay_masked = essay_embeddings * essay_mask.unsqueeze(-1)
            essay_pooled = essay_masked.sum(dim=1) / essay_mask.sum(dim=1, keepdim=True).clamp(min=1)
        else:
            essay_pooled = essay_embeddings.mean(dim=1)
            
        if prompt_mask is not None:
            prompt_masked = prompt_embeddings * prompt_mask.unsqueeze(-1)
            prompt_pooled = prompt_masked.sum(dim=1) / prompt_mask.sum(dim=1, keepdim=True).clamp(min=1)
        else:
            prompt_pooled = prompt_embeddings.mean(dim=1)
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ä½œä¸ºåŸºç¡€
        cosine_sim = F.cosine_similarity(essay_pooled, prompt_pooled, dim=1).unsqueeze(1)
        
        # ç»„åˆç‰¹å¾
        combined = torch.cat([essay_pooled, prompt_pooled], dim=1)
        
        # è®¡ç®—ç›¸å…³åº¦åˆ†æ•° - åŸºäºç»„åˆç‰¹å¾å’Œä½™å¼¦ç›¸ä¼¼åº¦
        base_relevance = self.similarity_net(combined).squeeze(-1)
        
        # ç»“åˆä½™å¼¦ç›¸ä¼¼åº¦ï¼Œé¿å…ç½‘ç»œé¥±å’Œ
        relevance_score = 0.3 * base_relevance + 0.7 * cosine_sim.squeeze(-1)
        
        return relevance_score, None

# ---------------------------
# ---- æ•°æ®åŠ è½½å’Œé¢„å¤„ç† ----
# ---------------------------
print("Loading dataset...")
df = pd.read_csv(DATASET_PATH)
df = df[df['score'].notna()]
df = df[df['score'] != 6]

print(f"Total essays: {len(df)} | unique prompts: {df['assignment'].nunique()}")
print("Score counts (before):")
print(df['score'].value_counts().sort_index())

# æ„å»ºpromptæ–‡æœ¬æ˜ å°„
prompt_texts = {}
for assignment in df['assignment'].unique():
    if assignment not in prompt_texts:
        prompt_texts[assignment] = f"Writing prompt: {assignment}"

print(f"Loaded {len(prompt_texts)} unique prompts")

# ========== ä¿®å¤æ•°æ®æ³„éœ²é—®é¢˜ ==========
# å…ˆåˆ’åˆ†æ•°æ®é›†ï¼Œå†åœ¨è®­ç»ƒé›†ä¸Šè¿›è¡Œå¹³è¡¡é‡‡æ ·
print("\n=== ä¿®å¤æ•°æ®æ³„éœ²é—®é¢˜ ===")
print("å…ˆåˆ’åˆ†æ•°æ®é›†ï¼Œå†åœ¨è®­ç»ƒé›†ä¸Šè¿›è¡Œå¹³è¡¡é‡‡æ ·...")

# æŒ‰ç…§7:1:2åˆ’åˆ†è®­ç»ƒã€éªŒè¯ã€æµ‹è¯•é›†
train_val_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['score'])
train_df, val_df = train_test_split(train_val_df, test_size=0.125, random_state=42, stratify=train_val_df['score'])

print(f"åŸå§‹åˆ’åˆ† - Train: {len(train_df)} | Val: {len(val_df)} | Test: {len(test_df)}")

# åªåœ¨è®­ç»ƒé›†ä¸Šè¿›è¡Œå¹³è¡¡é‡‡æ ·
print("åªåœ¨è®­ç»ƒé›†ä¸Šè¿›è¡Œå¹³è¡¡é‡‡æ ·...")
score_counts_train = train_df['score'].value_counts()
max_count_train = score_counts_train.max()

train_df_balanced = pd.concat([
    train_df[train_df['score'] == s].sample(max_count_train, replace=True, random_state=42)
    for s in sorted(score_counts_train.index)
], ignore_index=True).sample(frac=1, random_state=42).reset_index(drop=True)

# ä½¿ç”¨å¹³è¡¡åçš„è®­ç»ƒé›†
train_df = train_df_balanced

print("Score counts after balancing (training set only):")
print(f"Train: {train_df['score'].value_counts().sort_index()}")
print(f"Val: {val_df['score'].value_counts().sort_index()}")
print(f"Test: {test_df['score'].value_counts().sort_index()}")

print(f"æœ€ç»ˆæ•°æ®é›†å¤§å° - Train: {len(train_df)} | Val: {len(val_df)} | Test: {len(test_df)}")

# å½’ä¸€åŒ–ç›®æ ‡ - åªåœ¨è®­ç»ƒé›†ä¸Šæ‹Ÿåˆ
scaler = MinMaxScaler()
train_df['score_norm'] = scaler.fit_transform(train_df[['score']])
val_df['score_norm'] = scaler.transform(val_df[['score']])
test_df['score_norm'] = scaler.transform(test_df[['score']])

# é‡ç½®æ‰€æœ‰æ•°æ®é›†çš„ç´¢å¼•ä»¥ç¡®ä¿ä¸€è‡´æ€§
train_df = train_df.reset_index(drop=True)
val_df = val_df.reset_index(drop=True)
test_df = test_df.reset_index(drop=True)

# é¢„ç¼“å­˜å›¾å’Œç‰¹å¾
def precompute_all(df_src, dataset_name=""):
    graph_map = {}
    feature_map = {}
    prompt_map = {}
    
    for idx, row in tqdm(df_src.iterrows(), total=len(df_src), desc=f"Precomputing {dataset_name}"):
        text = str(row['full_text'])
        
        # è·å–promptæ–‡æœ¬
        prompt_name = row['prompt_name'] if pd.notna(row['prompt_name']) else row['assignment']
        prompt_text = prompt_texts.get(prompt_name, f"Writing prompt: {prompt_name}")
        
        # ç¼“å­˜ä½œæ–‡å›¾å’Œç‰¹å¾
        h = text_hash(text)
        graph_path = os.path.join(GRAPH_CACHE_DIR, f"{h}.pt")
        feature_path = os.path.join(FEATURE_CACHE_DIR, f"{h}.npy")
        
        if not os.path.exists(graph_path):
            g = build_graph_from_text(text)
            torch.save(g, graph_path)
        if not os.path.exists(feature_path):
            features = feature_extractor.extract_features(text)
            np.save(feature_path, features)
            
        # ä½¿ç”¨é‡ç½®åçš„ç´¢å¼•ä½œä¸ºé”®
        graph_map[idx] = graph_path
        feature_map[idx] = feature_path
        
        # ç¼“å­˜promptç¼–ç 
        prompt_h = text_hash(prompt_text)
        prompt_path = os.path.join(PROMPT_CACHE_DIR, f"{prompt_h}.pt")
        if not os.path.exists(prompt_path):
            prompt_enc = tokenizer(
                prompt_text, 
                truncation=True, 
                padding='max_length', 
                max_length=128,
                return_tensors='pt'
            )
            torch.save(prompt_enc, prompt_path)
        prompt_map[idx] = prompt_path
            
    return graph_map, feature_map, prompt_map

print("Precomputing training data...")
train_graph_map, train_feature_map, train_prompt_map = precompute_all(train_df, "training data")
print("Precomputing validation data...")
val_graph_map, val_feature_map, val_prompt_map = precompute_all(val_df, "validation data")
print("Precomputing test data...")
test_graph_map, test_feature_map, test_prompt_map = precompute_all(test_df, "test data")

# ç‰¹å¾æ ‡å‡†åŒ– - åªåœ¨è®­ç»ƒé›†ä¸Šæ‹Ÿåˆï¼Œé¿å…æµ‹è¯•é›†æ³„éœ²
all_train_features = []
for i in range(len(train_df)):
    if i in train_feature_map:
        features = np.load(train_feature_map[i])
        all_train_features.append(features)

if len(all_train_features) > 0:
    all_train_features = np.array(all_train_features)
    feature_scaler = StandardScaler()
    feature_scaler.fit(all_train_features)
else:
    print("Warning: No training features found!")
    feature_scaler = StandardScaler()
    feature_scaler.fit(np.zeros((1, NUM_FEATURES)))

# ---------------------------
# ---- æ•°æ®é›†ç±» ----
# ---------------------------
class FusionGraphBertDataset(Dataset):
    def __init__(self, df, tokenizer, graph_map, feature_map, prompt_map, feature_scaler, is_training=False):
        self.df = df.reset_index(drop=True)
        self.tokenizer = tokenizer
        self.graph_map = graph_map
        self.feature_map = feature_map
        self.prompt_map = prompt_map
        self.feature_scaler = feature_scaler
        self.is_training = is_training

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        text = str(row['full_text'])
        
        # æ•°æ®å¢å¼º - åªåœ¨è®­ç»ƒæ—¶ä½¿ç”¨
        if self.is_training and random.random() < 0.1:
            # ç®€å•çš„æ•°æ®å¢å¼ºï¼šéšæœºåˆ é™¤ä¸€äº›å•è¯
            words = text.split()
            if len(words) > 10:
                # éšæœºåˆ é™¤5%çš„å•è¯
                num_to_remove = max(1, int(len(words) * 0.05))
                indices_to_remove = random.sample(range(len(words)), num_to_remove)
                words = [word for i, word in enumerate(words) if i not in indices_to_remove]
                text = ' '.join(words)
        
        # BERTç¼–ç 
        enc = self.tokenizer(text, truncation=True, padding='max_length', 
                           max_length=MAX_LENGTH, return_tensors='pt')
        input_ids = enc['input_ids'].squeeze(0)
        attention_mask = enc['attention_mask'].squeeze(0)
        
        # å›¾æ•°æ®
        if idx in self.graph_map:
            graph_path = self.graph_map[idx]
            graph = torch.load(graph_path, weights_only=False)
        else:
            graph = build_graph_from_text(text)
        
        # ç‰¹å¾æ•°æ®
        if idx in self.feature_map:
            feature_path = self.feature_map[idx]
            features = np.load(feature_path)
        else:
            features = feature_extractor.extract_features(text)
        
        features = self.feature_scaler.transform(features.reshape(1, -1)).squeeze()
        features = torch.tensor(features, dtype=torch.float32)
        
        # Promptç¼–ç 
        if idx in self.prompt_map:
            prompt_path = self.prompt_map[idx]
            prompt_enc = torch.load(prompt_path, weights_only=False)
        else:
            prompt_name = row['prompt_name'] if pd.notna(row['prompt_name']) else row['assignment']
            prompt_text = prompt_texts.get(prompt_name, f"Writing prompt: {prompt_name}")
            prompt_enc = tokenizer(
                prompt_text, 
                truncation=True, 
                padding='max_length', 
                max_length=128,
                return_tensors='pt'
            )
        
        prompt_ids = prompt_enc['input_ids'].squeeze(0)
        prompt_mask = prompt_enc['attention_mask'].squeeze(0)
        
        score = torch.tensor(row['score_norm'], dtype=torch.float32)
        
        return {
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "graph": graph,
            "features": features,
            "prompt_ids": prompt_ids,
            "prompt_mask": prompt_mask,
            "score": score
        }

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

train_dataset = FusionGraphBertDataset(train_df, tokenizer, train_graph_map, train_feature_map, train_prompt_map, feature_scaler, is_training=True)
val_dataset = FusionGraphBertDataset(val_df, tokenizer, val_graph_map, val_feature_map, val_prompt_map, feature_scaler)
test_dataset = FusionGraphBertDataset(test_df, tokenizer, test_graph_map, test_feature_map, test_prompt_map, feature_scaler)

# é‡‡æ ·å™¨ - åªåœ¨è®­ç»ƒé›†ä¸Šä½¿ç”¨
score_to_count = train_df['score'].value_counts().to_dict()
sample_weights = [1.0 / score_to_count[s] for s in train_df['score']]
sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)

def collate_fn(batch):
    input_ids = torch.stack([b['input_ids'] for b in batch]).to(DEVICE)
    attention_mask = torch.stack([b['attention_mask'] for b in batch]).to(DEVICE)
    
    # å›¾æ•°æ®
    graphs = []
    for b in batch:
        g = b['graph']
        if g.x is None or g.x.size(0) == 0:
            g.x = torch.zeros((1, GRAPH_IN_DIM), dtype=torch.float32)
            g.edge_index = torch.tensor([[0],[0]], dtype=torch.long)
        graphs.append(g)
    graph_batch = Batch.from_data_list(graphs).to(DEVICE)
    
    # ç‰¹å¾æ•°æ®
    features = torch.stack([b['features'] for b in batch]).to(DEVICE)
    
    # Promptæ•°æ®
    prompt_ids = torch.stack([b['prompt_ids'] for b in batch]).to(DEVICE)
    prompt_mask = torch.stack([b['prompt_mask'] for b in batch]).to(DEVICE)
    
    scores = torch.stack([b['score'] for b in batch]).to(DEVICE)
    
    return {
        "input_ids": input_ids,
        "attention_mask": attention_mask,
        "graph": graph_batch,
        "features": features,
        "prompt_ids": prompt_ids,
        "prompt_mask": prompt_mask,
        "score": scores
    }

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=sampler, 
                         collate_fn=collate_fn, num_workers=0, pin_memory=False)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, 
                       collate_fn=collate_fn, num_workers=0, pin_memory=False)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False,
                        collate_fn=collate_fn, num_workers=0, pin_memory=False)

# ---------------------------
# ---- èåˆæ¨¡å‹ ----
# ---------------------------
class RobustFeatureWeightingModule(nn.Module):
    """ç¨³å¥çš„ç‰¹å¾åŠ æƒæ¨¡å—"""
    def __init__(self, num_features, hidden_dim=64):
        super().__init__()
        self.attention = nn.Sequential(
            nn.Linear(num_features, hidden_dim),
            nn.ReLU(),
            nn.Dropout(DROPOUT),
            nn.Linear(hidden_dim, num_features),
            nn.Sigmoid()
        )
        
        # åˆå§‹åŒ–
        self._init_weights()
        
    def _init_weights(self):
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0.5)  # åˆå§‹åå‘1.0
    
    def forward(self, features):
        weights = self.attention(features)
        weighted_features = features * weights
        return weighted_features, weights

class GraphEncoder(nn.Module):
    """å›¾ç¼–ç å™¨"""
    def __init__(self, in_dim=GRAPH_IN_DIM, hid=GRAPH_HIDDEN, out_dim=GRAPH_OUT, dropout=DROPOUT):
        super().__init__()
        self.conv1 = GraphConv(in_dim, hid)
        self.conv2 = GraphConv(hid, out_dim)
        self.conv3 = GraphConv(out_dim, out_dim // 2)
        self.norm1 = nn.LayerNorm(hid)
        self.norm2 = nn.LayerNorm(out_dim)
        self.norm3 = nn.LayerNorm(out_dim // 2)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, edge_index, batch):
        x = F.relu(self.norm1(self.conv1(x, edge_index)))
        x = F.relu(self.norm2(self.conv2(x, edge_index)))
        x = self.dropout(F.relu(self.norm3(self.conv3(x, edge_index))))
        g = global_mean_pool(x, batch)
        return g

class FusionGraphBertAES(nn.Module):
    def __init__(self, model_name=MODEL_NAME, graph_out=GRAPH_OUT, 
                 num_features=NUM_FEATURES, hidden_dim=512, dropout=DROPOUT,
                 prompt_relevance_weight=PROMPT_RELEVANCE_WEIGHT):
        super().__init__()
        
        self.prompt_relevance_weight = prompt_relevance_weight
        
        # BERTç¼–ç å™¨
        self.bert = AutoModel.from_pretrained(model_name)
        bert_hidden = self.bert.config.hidden_size
        
        # å›¾ç¼–ç å™¨
        self.graph_encoder = GraphEncoder(in_dim=GRAPH_IN_DIM, hid=GRAPH_HIDDEN, 
                                        out_dim=graph_out, dropout=dropout)
        
        # ç‰¹å¾åŠ æƒæ¨¡å— - ä½¿ç”¨ç¨³å¥ç‰ˆæœ¬
        self.feature_weighting = RobustFeatureWeightingModule(num_features)
        
        # Promptç›¸å…³åº¦æ¨¡å— - ä½¿ç”¨ä¿®å¤ç‰ˆæœ¬
        self.prompt_relevance = FixedPromptRelevanceModule(hidden_dim=bert_hidden, dropout=dropout)
        
        # èåˆå±‚
        self.fusion_input_dim = bert_hidden + graph_out + num_features
        print(f"ğŸ”§ Model initialization:")
        print(f"   - BERT hidden size: {bert_hidden}")
        print(f"   - Graph output size: {graph_out}")
        print(f"   - Feature size: {num_features}")
        print(f"   - Prompt relevance weight: {prompt_relevance_weight}")
        print(f"   - Total fusion input: {self.fusion_input_dim}")
        
        self.fusion_layers = nn.Sequential(
            nn.Linear(self.fusion_input_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, 1)
        )
        
        # å†»ç»“éƒ¨åˆ†BERTå±‚
        if FREEZE_BERT_LAYERS > 0:
            for i, layer in enumerate(self.bert.encoder.layer):
                if i < FREEZE_BERT_LAYERS:
                    for param in layer.parameters():
                        param.requires_grad = False

    def forward(self, input_ids, attention_mask, graph: Batch, features, prompt_ids, prompt_mask):
        # BERTç¼–ç  - ä½œæ–‡
        bert_out = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        essay_embeddings = bert_out.last_hidden_state  # [batch_size, seq_len, hidden_dim]
        bert_pooled = essay_embeddings[:, 0, :]  # ä½¿ç”¨[CLS] token
        
        # BERTç¼–ç  - Prompt (éœ€è¦æ¢¯åº¦ï¼Œå› ä¸ºç›¸å…³åº¦æ¨¡å—éœ€è¦è®­ç»ƒ)
        prompt_out = self.bert(input_ids=prompt_ids, attention_mask=prompt_mask)
        prompt_embeddings = prompt_out.last_hidden_state  # [batch_size, prompt_len, hidden_dim]
        
        # å›¾ç¼–ç 
        graph_emb = self.graph_encoder(graph.x, graph.edge_index, graph.batch)
        
        # ç‰¹å¾åŠ æƒ
        weighted_features, feature_weights = self.feature_weighting(features)
        
        # Promptç›¸å…³åº¦è®¡ç®— - ä¿®å¤ç‰ˆæœ¬
        relevance_score, _ = self.prompt_relevance(
            essay_embeddings, prompt_embeddings, attention_mask, prompt_mask
        )
        
        # ç‰¹å¾èåˆ
        x = torch.cat([bert_pooled, graph_emb, weighted_features], dim=1)
        
        # æ£€æŸ¥ç»´åº¦æ˜¯å¦åŒ¹é…
        if x.shape[1] != self.fusion_input_dim:
            self._adapt_fusion_layer(x.shape[1])
        
        # åŸºç¡€è¯„åˆ†
        base_score = self.fusion_layers(x).squeeze(-1)
        
        # ç»“åˆPromptç›¸å…³åº¦è°ƒæ•´æœ€ç»ˆè¯„åˆ† - ä½¿ç”¨æ›´åˆç†çš„è°ƒæ•´æ–¹å¼
        # ç›¸å…³åº¦åˆ†æ•°ä½œä¸ºç½®ä¿¡åº¦æƒé‡
        relevance_weight = 1 + (relevance_score - 0.5) * self.prompt_relevance_weight * 50
        final_score = base_score * relevance_weight
        
        # ç¡®ä¿è¯„åˆ†åœ¨åˆç†èŒƒå›´å†…
        final_score = torch.clamp(final_score, 0.0, 1.0)
        
        return final_score, feature_weights, relevance_score, base_score
    
    def _adapt_fusion_layer(self, actual_dim):
        """åŠ¨æ€è°ƒæ•´èåˆå±‚ä»¥é€‚åº”å®é™…è¾“å…¥ç»´åº¦"""
        print(f"ğŸ› ï¸ Adapting fusion layer to input dimension: {actual_dim}")
        old_layer = self.fusion_layers[0]
        self.fusion_layers[0] = nn.Linear(actual_dim, old_layer.out_features).to(DEVICE)
        self.fusion_input_dim = actual_dim

model = FusionGraphBertAES().to(DEVICE)

# ---------------------------
# ---- ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•° ----
# ---------------------------
class StableMultiTaskLoss(nn.Module):
    """ç¨³å®šçš„å¤šä»»åŠ¡æŸå¤±å‡½æ•°"""
    def __init__(self, alpha=0.95, beta=0.03, gamma=0.02):
        super().__init__()
        self.alpha = alpha  # å›å½’æŸå¤±æƒé‡
        self.beta = beta    # ç‰¹å¾ç¨€ç–æ€§æŸå¤±æƒé‡
        self.gamma = gamma  # ç›¸å…³åº¦æ­£åˆ™åŒ–æŸå¤±æƒé‡
        self.regression_loss = nn.SmoothL1Loss()
        
    def forward(self, preds, targets, feature_weights, relevance_scores):
        reg_loss = self.regression_loss(preds, targets)
        
        # æ›´æ¸©å’Œçš„ç‰¹å¾ç¨€ç–æ€§æŸå¤±
        sparse_loss = torch.mean(torch.relu(0.05 - feature_weights))  # é¼“åŠ±æƒé‡è‡³å°‘ä¸º0.05
        
        # æ›´æ¸©å’Œçš„ç›¸å…³åº¦æ­£åˆ™åŒ–ï¼šé¼“åŠ±ç›¸å…³åº¦åˆ†æ•°åœ¨0.3-0.7ä¹‹é—´
        relevance_mean = torch.mean(relevance_scores)
        # ä½¿ç”¨æ›´å¹³æ»‘çš„æŸå¤±å‡½æ•°ï¼Œé¿å…è¿‡åº¦æƒ©ç½š
        relevance_loss = torch.abs(relevance_mean - 0.5)
        
        total_loss = self.alpha * reg_loss + self.beta * sparse_loss + self.gamma * relevance_loss
        
        return total_loss, reg_loss, sparse_loss, relevance_loss

# ä¼˜åŒ–å™¨åˆ†ç»„
no_decay = ["bias", "LayerNorm.weight"]
optimizer_grouped_parameters = [
    # BERTå‚æ•°
    {
        "params": [p for n, p in model.bert.named_parameters() if p.requires_grad],
        "lr": LR_BERT,
        "weight_decay": 0.01
    },
    # å›¾ç¼–ç å™¨å‚æ•°
    {
        "params": [p for n, p in model.graph_encoder.named_parameters() if p.requires_grad],
        "lr": LR_GRAPH,
        "weight_decay": 0.01
    },
    # ç‰¹å¾åŠ æƒå‚æ•°
    {
        "params": [p for n, p in model.feature_weighting.named_parameters() if p.requires_grad],
        "lr": LR_FEATURES,
        "weight_decay": 0.001
    },
    # Promptç›¸å…³åº¦å‚æ•° - é™ä½å­¦ä¹ ç‡ï¼Œé¿å…é¥±å’Œ
    {
        "params": [p for n, p in model.prompt_relevance.named_parameters() if p.requires_grad],
        "lr": LR_FEATURES * 0.5,  # é™ä½å­¦ä¹ ç‡
        "weight_decay": 0.001
    },
    # èåˆå±‚å‚æ•°
    {
        "params": [p for n, p in model.fusion_layers.named_parameters() if p.requires_grad],
        "lr": LR_GRAPH,
        "weight_decay": 0.01
    },
]

optimizer = AdamW(optimizer_grouped_parameters)
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)
criterion = StableMultiTaskLoss()

# ---------------------------
# ---- è®­ç»ƒå¾ªç¯ ----
# ---------------------------
class EarlyStopping:
    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt'):
        self.patience = patience
        self.verbose = verbose
        self.counter = 0
        self.best_score = None
        self.early_stop = False
        self.val_loss_min = np.inf
        self.delta = delta
        self.path = path

    def __call__(self, val_loss, model):
        score = -val_loss

        if self.best_score is None:
            self.best_score = score
            self.save_checkpoint(val_loss, model)
        elif score < self.best_score + self.delta:
            self.counter += 1
            if self.verbose:
                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_score = score
            self.save_checkpoint(val_loss, model)
            self.counter = 0

    def save_checkpoint(self, val_loss, model):
        if self.verbose:
            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving model...')
        torch.save(model.state_dict(), self.path)
        self.val_loss_min = val_loss

# æ··åˆç²¾åº¦è®­ç»ƒ - æ›´æ–°ä¸ºæ–°çš„API
use_amp = DEVICE.type == "cuda" and torch.cuda.is_available()
if use_amp:
    scaler = torch.cuda.amp.GradScaler()
    print("Using mixed precision training")
else:
    scaler = None
    print("Mixed precision not available, using standard training")

early_stopping = EarlyStopping(patience=PATIENCE, verbose=True, path="fusion_graphbert_best.pth")

# è®­ç»ƒè®°å½•
train_loss_history, val_loss_history, val_qwk_history = [], [], []
best_val_loss = float("inf")

# ä¿®å¤çš„å¥åº·æ£€æŸ¥å‡½æ•°
def detailed_health_check(model, batch, step):
    """ä¿®å¤çš„å¥åº·æ£€æŸ¥ - é¿å…ç»´åº¦ä¸åŒ¹é…é”™è¯¯"""
    model.eval()
    with torch.no_grad():
        input_ids = batch["input_ids"]
        attention_mask = batch["attention_mask"]
        graph = batch["graph"]
        features = batch["features"]
        prompt_ids = batch["prompt_ids"]
        prompt_mask = batch["prompt_mask"]
        
        # å‰å‘ä¼ æ’­åˆ°å„æ¨¡å—
        bert_out = model.bert(input_ids=input_ids, attention_mask=attention_mask)
        essay_embeddings = bert_out.last_hidden_state
        
        prompt_out = model.bert(input_ids=prompt_ids, attention_mask=prompt_mask)
        prompt_embeddings = prompt_out.last_hidden_state
        
        # æ£€æŸ¥ç‰¹å¾åŠ æƒ
        weighted_features, feature_weights = model.feature_weighting(features)
        
        # æ£€æŸ¥ç›¸å…³åº¦
        relevance_score, _ = model.prompt_relevance(
            essay_embeddings, prompt_embeddings, attention_mask, prompt_mask
        )
        
        # æ£€æŸ¥åŸºç¡€è¯„åˆ† - ä½¿ç”¨å®Œæ•´çš„æ¨¡å‹å‰å‘ä¼ æ’­
        _, _, relevance_scores, base_scores = model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            graph=graph,
            features=features,
            prompt_ids=prompt_ids,
            prompt_mask=prompt_mask
        )
        
        if step % 500 == 0:
            print(f"\nğŸ” Step {step} - è¯¦ç»†å¥åº·æ£€æŸ¥:")
            print(f"   ç‰¹å¾æƒé‡ - å‡å€¼: {feature_weights.mean().item():.6f}, èŒƒå›´: [{feature_weights.min().item():.6f}, {feature_weights.max().item():.6f}]")
            print(f"   ç›¸å…³åº¦åˆ†æ•° - å‡å€¼: {relevance_score.mean().item():.6f}, èŒƒå›´: [{relevance_score.min().item():.6f}, {relevance_score.max().item():.6f}]")
            print(f"   åŸºç¡€è¯„åˆ† - å‡å€¼: {base_scores.mean().item():.6f}, èŒƒå›´: [{base_scores.min().item():.6f}, {base_scores.max().item():.6f}]")
            print(f"   åŠ æƒç‰¹å¾ - å‡å€¼: {weighted_features.mean().item():.6f}")
            
            # æ£€æŸ¥æ¢¯åº¦æƒ…å†µ
            for name, param in model.prompt_relevance.named_parameters():
                if param.grad is not None:
                    grad_mean = param.grad.abs().mean().item()
                    if grad_mean > 0:
                        print(f"   {name} - æ¢¯åº¦å‡å€¼: {grad_mean:.6f}")
    
    model.train()

print("ğŸš€ Starting training with fixed modules...")

for epoch in range(1, EPOCHS + 1):
    model.train()
    total_train_loss = 0.0
    total_reg_loss = 0.0
    total_sparse_loss = 0.0
    total_relevance_loss = 0.0
    
    optimizer.zero_grad()
    pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f"Epoch {epoch}/{EPOCHS}")
    
    for step, batch in pbar:
        # å®šæœŸæ£€æŸ¥æ¨¡å—å¥åº·çŠ¶æ€
        if step % 500 == 0:
            detailed_health_check(model, batch, step)
        
        input_ids = batch["input_ids"]
        attention_mask = batch["attention_mask"]
        graph = batch["graph"]
        features = batch["features"]
        prompt_ids = batch["prompt_ids"]
        prompt_mask = batch["prompt_mask"]
        target = batch["score"]

        if use_amp:
            # ä½¿ç”¨æ–°çš„autocast API
            with torch.amp.autocast(device_type='cuda'):
                preds, feature_weights, relevance_scores, base_scores = model(
                    input_ids=input_ids, 
                    attention_mask=attention_mask, 
                    graph=graph, 
                    features=features,
                    prompt_ids=prompt_ids,
                    prompt_mask=prompt_mask
                )
                loss, reg_loss, sparse_loss, relevance_loss = criterion(preds, target, feature_weights, relevance_scores)
                loss = loss / ACCUM_STEPS

            scaler.scale(loss).backward()

            if (step + 1) % ACCUM_STEPS == 0:
                scaler.step(optimizer)
                scaler.update()
                optimizer.zero_grad()
        else:
            # ä¸ä½¿ç”¨æ··åˆç²¾åº¦
            preds, feature_weights, relevance_scores, base_scores = model(
                input_ids=input_ids, 
                attention_mask=attention_mask, 
                graph=graph, 
                features=features,
                prompt_ids=prompt_ids,
                prompt_mask=prompt_mask
            )
            loss, reg_loss, sparse_loss, relevance_loss = criterion(preds, target, feature_weights, relevance_scores)
            loss = loss / ACCUM_STEPS

            loss.backward()

            if (step + 1) % ACCUM_STEPS == 0:
                optimizer.step()
                optimizer.zero_grad()

        total_train_loss += loss.item() * ACCUM_STEPS
        total_reg_loss += reg_loss.item() * ACCUM_STEPS
        total_sparse_loss += sparse_loss.item() * ACCUM_STEPS
        total_relevance_loss += relevance_loss.item() * ACCUM_STEPS
        
        # æ˜¾ç¤ºç›¸å…³åº¦ç»Ÿè®¡ä¿¡æ¯
        avg_relevance = relevance_scores.mean().item()
        std_relevance = relevance_scores.std().item()
        avg_feature_weights = feature_weights.mean().item()
        
        pbar.set_postfix({
            "total_loss": f"{(total_train_loss / (step+1)):.4f}",
            "reg_loss": f"{(total_reg_loss / (step+1)):.4f}",
            "sparse_loss": f"{(total_sparse_loss / (step+1)):.4f}",
            "rel_loss": f"{(total_relevance_loss / (step+1)):.4f}",
            "rel_mean": f"{avg_relevance:.3f}",
            "feat_w": f"{avg_feature_weights:.3f}"
        })

    avg_train_loss = total_train_loss / len(train_loader)
    train_loss_history.append(avg_train_loss)

    # éªŒè¯
    model.eval()
    val_preds, val_true, val_relevance = [], [], []
    total_val_loss = 0.0
    
    with torch.no_grad():
        for batch in tqdm(val_loader, desc="Validating"):
            input_ids = batch["input_ids"]
            attention_mask = batch["attention_mask"]
            graph = batch["graph"]
            features = batch["features"]
            prompt_ids = batch["prompt_ids"]
            prompt_mask = batch["prompt_mask"]
            target = batch["score"]

            if use_amp:
                # ä½¿ç”¨æ–°çš„autocast API
                with torch.amp.autocast(device_type='cuda'):
                    preds, feature_weights, relevance_scores, base_scores = model(
                        input_ids=input_ids, 
                        attention_mask=attention_mask, 
                        graph=graph, 
                        features=features,
                        prompt_ids=prompt_ids,
                        prompt_mask=prompt_mask
                    )
                    loss, _, _, _ = criterion(preds, target, feature_weights, relevance_scores)
            else:
                preds, feature_weights, relevance_scores, base_scores = model(
                    input_ids=input_ids, 
                    attention_mask=attention_mask, 
                    graph=graph, 
                    features=features,
                    prompt_ids=prompt_ids,
                    prompt_mask=prompt_mask
                )
                loss, _, _, _ = criterion(preds, target, feature_weights, relevance_scores)

            total_val_loss += loss.item()
            val_preds.extend(preds.detach().cpu().numpy())
            val_true.extend(target.detach().cpu().numpy())
            val_relevance.extend(relevance_scores.detach().cpu().numpy())

    avg_val_loss = total_val_loss / len(val_loader)
    val_loss_history.append(avg_val_loss)
    scheduler.step(avg_val_loss)

    # è®¡ç®—QWK
    min_score = df['score'].min()
    max_score = df['score'].max()
    
    val_true_denorm = (np.array(val_true) * (max_score - min_score)) + min_score
    val_pred_denorm = (np.array(val_preds) * (max_score - min_score)) + min_score

    val_true_round = np.round(val_true_denorm).astype(int)
    val_pred_round = np.clip(np.round(val_pred_denorm), min_score, max_score).astype(int)

    qwk = cohen_kappa_score(val_true_round, val_pred_round, weights='quadratic')
    val_qwk_history.append(qwk)

    # è®¡ç®—ç›¸å…³åº¦ç»Ÿè®¡
    avg_relevance = np.mean(val_relevance)
    std_relevance = np.std(val_relevance)
    
    print(f"\nEpoch {epoch} Summary:")
    print(f"Train Loss: {avg_train_loss:.5f} | Val Loss: {avg_val_loss:.5f} | QWK: {qwk:.4f}")
    print(f"Prompt Relevance - Mean: {avg_relevance:.4f} | Std: {std_relevance:.4f}")
    print(f"Learning Rate: {optimizer.param_groups[0]['lr']:.2e}")

    # æ—©åœ
    early_stopping(avg_val_loss, model)
    if early_stopping.early_stop:
        print("â›” Early stopping triggered.")
        break

# ---------------------------
# ---- è¯„ä¼°å’Œå¯è§†åŒ– ----
# ---------------------------
print("\n--- Generating Final Evaluation ---\n")

# åŠ è½½æœ€ä½³æ¨¡å‹
model.load_state_dict(torch.load("fusion_graphbert_best.pth"))
model.eval()

# éªŒè¯é›†è¯„ä¼°
final_val_preds, final_val_true, final_val_relevance = [], [], []
feature_weights_all = []

with torch.no_grad():
    for batch in val_loader:
        input_ids = batch["input_ids"]
        attention_mask = batch["attention_mask"]
        graph = batch["graph"]
        features = batch["features"]
        prompt_ids = batch["prompt_ids"]
        prompt_mask = batch["prompt_mask"]
        target = batch["score"]
        
        if use_amp:
            # ä½¿ç”¨æ–°çš„autocast API
            with torch.amp.autocast(device_type='cuda'):
                preds, feature_weights, relevance_scores, base_scores = model(
                    input_ids=input_ids, 
                    attention_mask=attention_mask, 
                    graph=graph, 
                    features=features,
                    prompt_ids=prompt_ids,
                    prompt_mask=prompt_mask
                )
        else:
            preds, feature_weights, relevance_scores, base_scores = model(
                input_ids=input_ids, 
                attention_mask=attention_mask, 
                graph=graph, 
                features=features,
                prompt_ids=prompt_ids,
                prompt_mask=prompt_mask
            )
        
        final_val_preds.extend(preds.detach().cpu().numpy())
        final_val_true.extend(target.detach().cpu().numpy())
        final_val_relevance.extend(relevance_scores.detach().cpu().numpy())
        feature_weights_all.extend(feature_weights.detach().cpu().numpy())

# åå½’ä¸€åŒ–
final_y_true_val = (np.array(final_val_true) * (max_score - min_score)) + min_score
final_y_pred_continuous_val = (np.array(final_val_preds) * (max_score - min_score)) + min_score
final_y_pred_rounded_val = np.clip(final_y_pred_continuous_val.round(), min_score, max_score).astype(int)

# è®¡ç®—éªŒè¯é›†æŒ‡æ ‡
final_mse_val = mean_squared_error(final_y_true_val, final_y_pred_continuous_val)
final_mae_val = mean_absolute_error(final_y_true_val, final_y_pred_continuous_val)
final_qwk_val = cohen_kappa_score(final_y_true_val.astype(int), final_y_pred_rounded_val, weights='quadratic')

print(f"\n=== Validation Set Evaluation Results ===")
print(f"QWK: {final_qwk_val:.4f}")
print(f"MSE: {final_mse_val:.4f}")
print(f"MAE: {final_mae_val:.4f}")
print(f"Prompt Relevance - Mean: {np.mean(final_val_relevance):.4f} | Std: {np.std(final_val_relevance):.4f}")

# æµ‹è¯•é›†è¯„ä¼°
test_preds, test_true, test_relevance = [], [], []
test_feature_weights_all = []

with torch.no_grad():
    for batch in tqdm(test_loader, desc="Testing"):
        input_ids = batch["input_ids"]
        attention_mask = batch["attention_mask"]
        graph = batch["graph"]
        features = batch["features"]
        prompt_ids = batch["prompt_ids"]
        prompt_mask = batch["prompt_mask"]
        target = batch["score"]
        
        if use_amp:
            # ä½¿ç”¨æ–°çš„autocast API
            with torch.amp.autocast(device_type='cuda'):
                preds, feature_weights, relevance_scores, base_scores = model(
                    input_ids=input_ids, 
                    attention_mask=attention_mask, 
                    graph=graph, 
                    features=features,
                    prompt_ids=prompt_ids,
                    prompt_mask=prompt_mask
                )
        else:
            preds, feature_weights, relevance_scores, base_scores = model(
                input_ids=input_ids, 
                attention_mask=attention_mask, 
                graph=graph, 
                features=features,
                prompt_ids=prompt_ids,
                prompt_mask=prompt_mask
            )
        
        test_preds.extend(preds.detach().cpu().numpy())
        test_true.extend(target.detach().cpu().numpy())
        test_relevance.extend(relevance_scores.detach().cpu().numpy())
        test_feature_weights_all.extend(feature_weights.detach().cpu().numpy())

# åå½’ä¸€åŒ–
test_true_denorm = (np.array(test_true) * (max_score - min_score)) + min_score
test_pred_continuous = (np.array(test_preds) * (max_score - min_score)) + min_score
test_pred_rounded = np.clip(test_pred_continuous.round(), min_score, max_score).astype(int)

# è®¡ç®—æµ‹è¯•é›†æŒ‡æ ‡
test_mse = mean_squared_error(test_true_denorm, test_pred_continuous)
test_mae = mean_absolute_error(test_true_denorm, test_pred_continuous)
test_qwk = cohen_kappa_score(test_true_denorm.astype(int), test_pred_rounded, weights='quadratic')

print(f"\n=== Test Set Evaluation Results ===")
print(f"QWK: {test_qwk:.4f}")
print(f"MSE: {test_mse:.4f}")
print(f"MAE: {test_mae:.4f}")
print(f"Prompt Relevance - Mean: {np.mean(test_relevance):.4f} | Std: {np.std(test_relevance):.4f}")

# åˆ†æç›¸å…³åº¦ä¸è¯„åˆ†çš„å…³ç³»
if len(test_relevance) > 1:
    relevance_corr = np.corrcoef(test_relevance, test_true_denorm)[0, 1]
    print(f"Correlation between relevance and true score: {relevance_corr:.4f}")
else:
    relevance_corr = 0
    print("Not enough samples to calculate correlation")

# ä¿å­˜æµ‹è¯•é›†é¢„æµ‹ç»“æœ
test_results_df = pd.DataFrame({
    'true_score': test_true_denorm,
    'pred_score_continuous': test_pred_continuous,
    'pred_score_rounded': test_pred_rounded,
    'prompt_relevance': test_relevance
})
test_results_df.to_csv('test_set_predictions.csv', index=False)
print(f"\nTest set predictions saved to: test_set_predictions.csv")

print(f"\n=== Final Results Summary ===")
print(f"Validation Set - QWK: {final_qwk_val:.4f}, MSE: {final_mse_val:.4f}, MAE: {final_mae_val:.4f}")
print(f"Test Set - QWK: {test_qwk:.4f}, MSE: {test_mse:.4f}, MAE: {test_mae:.4f}")
print(f"Prompt Relevance - Mean: {np.mean(test_relevance):.4f}, Std: {np.std(test_relevance):.4f}" if len(test_relevance) > 0 else "No relevance data")
print(f"Correlation between relevance and true score: {relevance_corr:.4f}")

print("\nğŸ¯ Model training completed successfully!")

print("\nğŸ’¾ Saving model and related files...")

# å®šä¹‰ä¿å­˜è·¯å¾„
model_path = "fusion_graphbert_best.pth"
feature_scaler_path = "feature_scaler.pkl"
target_scaler_path = "target_scaler.pkl"
config_path = "model_config.json"
tokenizer_path = "./tokenizer"

# 1. ä¿å­˜æ¨¡å‹æƒé‡ï¼ˆå·²ç»é€šè¿‡æ—©åœä¿å­˜ï¼Œè¿™é‡Œç¡®ä¿å­˜åœ¨ï¼‰
if os.path.exists(model_path):
    print(f"âœ“ Model weights saved at: {model_path}")
else:
    # å¦‚æœæ²¡æœ‰ä¿å­˜ï¼Œé‡æ–°ä¿å­˜ä¸€æ¬¡
    torch.save(model.state_dict(), model_path)
    print(f"âœ“ Model weights saved at: {model_path}")

# 2. ä¿å­˜ç‰¹å¾ç¼©æ”¾å™¨
try:
    with open(feature_scaler_path, 'wb') as f:
        pickle.dump(feature_scaler, f)
    print(f"âœ“ Feature scaler saved at: {feature_scaler_path}")
except Exception as e:
    print(f"âœ— Failed to save feature scaler: {e}")

# 3. ä¿å­˜ç›®æ ‡ç¼©æ”¾å™¨
try:
    with open(target_scaler_path, 'wb') as f:
        pickle.dump(scaler, f)
    print(f"âœ“ Target scaler saved at: {target_scaler_path}")
except Exception as e:
    print(f"âœ— Failed to save target scaler: {e}")

# 4. ä¿å­˜æ¨¡å‹é…ç½®
model_config = {
    "model_name": MODEL_NAME,
    "max_length": MAX_LENGTH,
    "graph_in_dim": GRAPH_IN_DIM,
    "graph_hidden": GRAPH_HIDDEN,
    "graph_out": GRAPH_OUT,
    "num_features": NUM_FEATURES,
    "dropout": DROPOUT,
    "freeze_bert_layers": FREEZE_BERT_LAYERS,
    "prompt_relevance_weight": PROMPT_RELEVANCE_WEIGHT,
    "score_min": float(df['score'].min()),
    "score_max": float(df['score'].max()),
    "feature_names": [
        "char_count", "word_count", "comma_count", "unique_words", 
        "proper_nouns", "determiners", "nouns", "adverbs", 
        "adjectives", "prepositions", "gunning_fog", "smog_index", 
        "rix_index", "dale_chall", "word_types", "sentence_count", 
        "long_words", "complex_words", "non_common_words", "difficult_words"
    ],
    "training_info": {
        "train_size": len(train_df),
        "val_size": len(val_df),
        "test_size": len(test_df),
        "epochs_trained": epoch,
        "best_val_qwk": float(final_qwk_val),
        "test_qwk": float(test_qwk),
        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    }
}

try:
    with open(config_path, 'w', encoding='utf-8') as f:
        json.dump(model_config, f, indent=2, ensure_ascii=False)
    print(f"âœ“ Model config saved at: {config_path}")
except Exception as e:
    print(f"âœ— Failed to save model config: {e}")

# 5. ä¿å­˜tokenizer
try:
    # ç¡®ä¿tokenizerç›®å½•å­˜åœ¨
    os.makedirs(tokenizer_path, exist_ok=True)
    
    # ä¿å­˜tokenizer
    tokenizer.save_pretrained(tokenizer_path)
    print(f"âœ“ Tokenizer saved at: {tokenizer_path}")
    
    # éªŒè¯tokenizerå¯ä»¥é‡æ–°åŠ è½½
    test_tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)
    print("âœ“ Tokenizer reload test successful")
except Exception as e:
    print(f"âœ— Failed to save tokenizer: {e}")

# 6. ä¿å­˜å®Œæ•´çš„æ¨¡å‹ï¼ˆå¯é€‰ï¼Œç”¨äºæ¨ç†ï¼‰
try:
    # ä¿å­˜å®Œæ•´æ¨¡å‹ç»“æ„ï¼ˆéœ€è¦åŒæ—¶ä¿å­˜æ¨¡å‹ç±»å’ŒçŠ¶æ€ï¼‰
    full_model_path = "fusion_graphbert_full.pth"
    torch.save({
        'model_state_dict': model.state_dict(),
        'model_config': model_config,
        'feature_scaler': feature_scaler,
        'target_scaler': scaler
    }, full_model_path)
    print(f"âœ“ Full model package saved at: {full_model_path}")
except Exception as e:
    print(f"âœ— Failed to save full model package: {e}")

# 7. ä¿å­˜è®­ç»ƒå†å²
training_history = {
    "train_loss": train_loss_history,
    "val_loss": val_loss_history,
    "val_qwk": val_qwk_history
}

try:
    history_path = "training_history.json"
    with open(history_path, 'w', encoding='utf-8') as f:
        json.dump(training_history, f, indent=2)
    print(f"âœ“ Training history saved at: {history_path}")
except Exception as e:
    print(f"âœ— Failed to save training history: {e}")

print("\nğŸ“ All model files have been saved successfully!")
print("Files created:")
print(f"  - Model weights: {model_path}")
print(f"  - Feature scaler: {feature_scaler_path}")
print(f"  - Target scaler: {target_scaler_path}")
print(f"  - Model config: {config_path}")
print(f"  - Tokenizer: {tokenizer_path}/")
print(f"  - Full model package: fusion_graphbert_full.pth")
print(f"  - Training history: training_history.json")
print(f"  - Test predictions: test_set_predictions.csv")